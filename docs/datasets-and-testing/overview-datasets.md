# Overview: Datasets

{% hint style="info" %}
Phoenix Datasets are currently in pre-release.
{% endhint %}

<figure><img src="https://storage.googleapis.com/arize-assets/phoenix/assets/images/evaluator.png" alt=""><figcaption><p>How Datasets are used to test changes to your AI application</p></figcaption></figure>

AI application development is bottlenecked by quality evaluations because engineers often face hard tradeoffs: which prompt or which LLM best balances performance, latency, and cost. High quality evaluations can help answer these questions with greater confidence.

## Why Evaluate

In AI development, it's hard to understand how a change will affect performance. This breaks the dev flow, making iteration more guesswork than engineering.

Evaluations solve this, helping distill the indeterminism of LLMs into effective feedback that helps you to ship a reliable product.

Specifically, good evals help you:

* Understand whether an update is an improvement or a regression
* Drill down into good / bad examples
* Compare specific examples vs. prior runs
* Avoid guesswork







\
