{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Scientific Agent Evaluation - Observe Agents Demo</h1>\n",
    "\n",
    "This tutorial demonstrates a **scientific approach to evaluating and improving AI agents** using Arize Phoenix. Based on the demo presented at **Arize Observe 2025**, this notebook shows how to apply rigorous evaluation methodologies to multi-agent systems.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "In this tutorial, we'll explore:\n",
    "\n",
    "1. **üîç Agent Observability**: How to trace and monitor multi-agent workflows using Phoenix\n",
    "2. **üìä Data-Driven Evaluation**: Creating systematic evaluation frameworks for agent performance\n",
    "3. **üß™ Scientific Methodology**: Using human annotations to build ground truth for LLM judges\n",
    "4. **üîÑ Iterative Improvement**: How to use evaluation results to improve agent performance through experimentation\n",
    "5. **üìà Performance Analysis**: Analyzing and comparing different agent configurations\n",
    "\n",
    "## üèóÔ∏è The Multi-Agent System\n",
    "\n",
    "Our demo features a **personal assistant system** with three specialized agents:\n",
    "\n",
    "- **üìÖ Calendar Agent**: Manages Google Calendar operations (scheduling, availability, events)\n",
    "- **üìß Mail Agent**: Handles Gmail operations (sending, reading, extracting event info)\n",
    "- **üéØ Coordinator Agent**: Orchestrates the workflow using handoffs between specialized agents\n",
    "\n",
    "**Example workflow**: \"Arrange a meeting with Bob next week\" ‚Üí find available slots ‚Üí draft invite ‚Üí send email ‚Üí book calendar\n",
    "\n",
    "## üî¨ Scientific Evaluation Approach\n",
    "\n",
    "This tutorial follows a **scientific methodology** for agent evaluation:\n",
    "\n",
    "1. **üéØ Define Success Metrics**: What makes an agent response \"helpful\" vs \"unhelpful\"\n",
    "2. **üìù Collect Ground Truth**: Use human annotations to establish evaluation standards\n",
    "3. **ü§ñ Build LLM Judges**: Create automated evaluators trained on human feedback\n",
    "4. **üß™ Run Experiments**: Test different agent configurations systematically\n",
    "5. **üìä Analyze Results**: Compare performance across different approaches\n",
    "\n",
    "‚ö†Ô∏è **Prerequisites**: This tutorial requires OpenAI API key and a running Phoenix instance\n",
    "\n",
    "Let's begin by setting up our environment and running some initial test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üõ†Ô∏è Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and set up our environment. This will install all the necessary packages for running our multi-agent system and connecting to Phoenix for observability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next:\n",
    "1. Set your environment variables by copying the `.env.example` file in this directory.\n",
    "2. Generate your google token file by running `python utils/generate_google_token.py`\n",
    "3. Setup the necessary prompts in Phoenix by running `python utils/setup_prompts.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Phase 1: Generate Agent Traces\n",
    "\n",
    "In this phase, we'll run our multi-agent system through a series of test cases to generate traces. Each test case represents a different type of user request that our personal assistant system should handle.\n",
    "\n",
    "These traces will serve as the foundation for our evaluation framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Environment and Project Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and set up project configuration\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "project_name = os.getenv(\"PHOENIX_PROJECT_NAME\", \"observe-agents\")\n",
    "print(f\"üìä Using Phoenix project: {project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Setup Phoenix Tracing\n",
    "\n",
    "Now we'll set up Phoenix tracing to capture all agent interactions. This includes:\n",
    "- Registering a tracer provider with Phoenix\n",
    "- Instrumenting the OpenAI Agents framework to automatically capture spans\n",
    "- Creating a tracer for manual span creation when needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Phoenix tracing and OpenAI Agents instrumentation\n",
    "import os\n",
    "\n",
    "from openinference.instrumentation.openai_agents import OpenAIAgentsInstrumentor\n",
    "\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(project_name=project_name, auto_instrument=False, verbose=False)\n",
    "\n",
    "OpenAIAgentsInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Run Test Cases Through the Multi-Agent System\n",
    "\n",
    "Now we'll execute our test cases through the coordinator agent.\n",
    "\n",
    "We're using a subset of 5 test cases to keep the demo manageable, but this approach scales to hundreds or thousands of test cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coordinator\n",
    "import pandas as pd\n",
    "from nest_asyncio import apply\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "\n",
    "apply()\n",
    "\n",
    "test_cases = pd.read_csv(\"utils/agent_inputs.csv\")[:5]\n",
    "\n",
    "for index, row in test_cases.iterrows():\n",
    "    print(f\"Running test case {index+1}\")\n",
    "    with tracer.start_as_current_span(\"run_agent\") as span:\n",
    "        span.set_attribute(\"input.value\", row[\"input\"])\n",
    "        span.set_attribute(\"openinference.span.kind\", \"CHAIN\")\n",
    "        result = coordinator.run_coordinator_agent(row[\"input\"])\n",
    "        span.set_attribute(\"output.value\", result)\n",
    "        span.set_status(Status(StatusCode.OK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è Pause - Add Annotations in the Phoenix UI\n",
    "\n",
    "Now that you have a few traces collected in Phoenix, you'll need to annotate them before continuing on in this notebook. You may need to create a new annotation config to complete this step.\n",
    "\n",
    "![adding-annotations](https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/annotation-example.gif)\n",
    "\n",
    "For more on annotations, [check out our docs](https://arize.com/docs/phoenix/tracing/features-tracing/how-to-annotate-traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "getpass(\"Have you added annotations to your traces in the Phoenix UI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìä Phase 2: Collect Human Annotations\n",
    "\n",
    "In this phase, we'll collect human annotations that will serve as ground truth for our evaluation system. This is a crucial step in our scientific approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Traces and Annotations from Phoenix\n",
    "\n",
    "We'll use the Phoenix client to retrieve our traces and any existing human annotations. This shows how Phoenix serves as both an observability platform and a data repository for evaluation workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "\n",
    "phoenix_client = Client(base_url=os.getenv(\"PHOENIX_BASE_URL\"))\n",
    "query = SpanQuery().where(\"name == 'run_agent'\")\n",
    "\n",
    "spans_df = phoenix_client.spans.get_spans_dataframe(project_identifier=project_name, query=query)\n",
    "spans_df.head()\n",
    "annotations_df = phoenix_client.spans.get_span_annotations_dataframe(\n",
    "    spans_dataframe=spans_df, project_identifier=project_name\n",
    ")\n",
    "\n",
    "combined_df = annotations_df.join(spans_df, how=\"inner\")\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expamples_df = combined_df[\n",
    "    [\"annotation_name\", \"result.label\", \"attributes.input.value\", \"attributes.output.value\"]\n",
    "].head()\n",
    "expamples_df = expamples_df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ü§ñ Phase 3: Build LLM Judge from Human Feedback\n",
    "\n",
    "This is where the scientific approach really shines. We'll create an LLM-based evaluator that's trained on human annotations, allowing us to scale human judgment to larger datasets.\n",
    "\n",
    "**What happens next:**\n",
    "- Extract human-annotated examples to use as few-shot examples\n",
    "- Design an evaluation prompt that teaches the LLM our quality criteria\n",
    "- Create a structured evaluation template with clear classification categories\n",
    "- Test the prompt to ensure it captures human judgment patterns\n",
    "\n",
    "This approach ensures our automated evaluations are grounded in human expertise rather than arbitrary criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Few-Shot Evaluation Prompt\n",
    "\n",
    "We'll build a prompt that incorporates human-annotated examples, teaching the LLM to evaluate agent responses according to our quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "examples_text = \"\\n\\n\".join(\n",
    "    [\n",
    "        f\"Request: {example['attributes.input.value']}\\nResponse: {example['attributes.output.value']}\\nHuman Rating: {example['result.label']}\"\n",
    "        for example in expamples_df.to_dict(orient=\"records\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_prompt = f\"\"\"\n",
    "You are an expert evaluator of agent responses. You will be given a human request and an agent's response.\n",
    "The current date is {datetime.now().strftime(\"%Y-%m-%d\")}. Be sure relative dates are correct.\n",
    "Your job is to evaluate whether the agent's response is helpful and effective using binary classification:\n",
    "- helpful: The response effectively addresses the request and provides useful information\n",
    "- somewhat_helpful: The response partially addresses the request but misses key points\n",
    "- unhelpful: The response doesn't address the request, is incorrect, or provides no useful information\n",
    "\n",
    "Here are some examples of helpful and unhelpful responses:\n",
    "{examples_text}\n",
    "\n",
    "## Evaluation\n",
    "Provide your answer in the following format:\n",
    "Request:\n",
    "Response:\n",
    "Explanation:\n",
    "Classification: helpful, somewhat_helpful, or unhelpful\n",
    "\n",
    "Request: {{attributes.input.value}}\n",
    "Response: {{attributes.output.value}}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Execute LLM-Based Evaluation\n",
    "\n",
    "Now you'll run our LLM judge across all the traces to generate consistent, scalable evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.evals.templates import PromptTemplate\n",
    "\n",
    "evals_df = llm_classify(\n",
    "    data=spans_df,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=[\"helpful\", \"somewhat_helpful\", \"unhelpful\"],\n",
    "    template=PromptTemplate(\n",
    "        template=eval_prompt,\n",
    "    ),\n",
    "    exit_on_error=False,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "## Assign 1 to helpful, 0.5 to somewhat helpful, and 0 to incorrect\n",
    "evals_df[\"score\"] = evals_df[\"label\"].apply(\n",
    "    lambda x: 1\n",
    "    if x is not None and x.lower() == \"helpful\"\n",
    "    else 0.5\n",
    "    if x is not None and x.lower() == \"somewhat_helpful\"\n",
    "    else 0\n",
    ")\n",
    "evals_df[[\"label\", \"score\", \"explanation\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Store Evaluations in Phoenix\n",
    "\n",
    "Finally, we'll log our evaluation results back to Phoenix, creating a complete feedback loop between observability and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client(endpoint=os.getenv(\"PHOENIX_BASE_URL\")).log_evaluations(\n",
    "    SpanEvaluations(\n",
    "        dataframe=evals_df,\n",
    "        eval_name=\"llm_agent_helpfulness\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üß™ Phase 4: Experimental Evaluation - Email Extraction Tool\n",
    "\n",
    "This phase demonstrates the power of systematic experimentation in agent improvement. Rather than making changes based on intuition, we'll use data-driven evaluation to compare different agent configurations.\n",
    "\n",
    "**What happens next:**\n",
    "- Run evaluations across multiple experimental configurations\n",
    "- Compare performance metrics between different agent setups\n",
    "- Identify which changes actually improve agent performance\n",
    "- Make evidence-based decisions about agent improvements\n",
    "\n",
    "This is the essence of **scientific agent development** - hypothesis, experiment, measure, iterate.\n",
    "\n",
    "For this example, we're going to zoom in on a particular tool and show how you can use experimentation techniques to improve performance on that tool. In this case, the tool we will use is the **email extraction tool**. \n",
    "\n",
    "You could apply these same techniques to the agent routing or any other tool used by the agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö†Ô∏è Pause - Run Experiment in Phoenix\n",
    "\n",
    "While you can run experiments in code, this example uses Phoenix's Prompt Playground to send a set of examples through different prompt versions.\n",
    "\n",
    "Follow along with the video below to see how:\n",
    "from IPython.display import HTML\n",
    "\n",
    "<video width=\"100%\" controls>\n",
    "  <source src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/videos/observe-experiment-example.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multiple Agent Configurations\n",
    "\n",
    "With that experiment run, you can now add different evaluation metrics to the results. These evaluations are calculated in the evaluate experiment file that's attached in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nest_asyncio import apply\n",
    "\n",
    "from utils.evaluate_experiment import run_evaluation_for_experiment\n",
    "\n",
    "apply()\n",
    "\n",
    "run_evaluation_for_experiment(\n",
    "    experiment_ids=[\"Add your Experiment IDs here\", \"Add as many as you want\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see experiment results that show improvement on the comparison against ground truth metric:\n",
    "![experiment-results](https://storage.googleapis.com/arize-phoenix-assets/assets/images/observe-improvement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Conclusion: The Scientific Agent Development Cycle\n",
    "\n",
    "Congratulations! You've just completed a full cycle of **scientific agent evaluation** using Arize Phoenix. Here's what you've learned:\n",
    "\n",
    "### üî¨ Key Takeaways\n",
    "\n",
    "1. **üìä Observability First**: Comprehensive tracing provides the foundation for understanding agent behavior\n",
    "2. **üë• Human-Centered Evaluation**: Ground truth from human annotators ensures meaningful quality metrics\n",
    "3. **ü§ñ Scalable Automation**: LLM judges trained on human feedback enable evaluation at scale\n",
    "4. **üß™ Systematic Experimentation**: A/B testing different agent configurations drives real improvements\n",
    "5. **üîÑ Continuous Improvement**: The feedback loop enables rapid iteration and problem-solving\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "To apply this methodology to your own agents:\n",
    "\n",
    "1. **Set up comprehensive tracing** for your agent system using Phoenix\n",
    "2. **Collect human annotations** on a representative sample of agent interactions\n",
    "3. **Build LLM judges** that capture your specific quality criteria\n",
    "4. **Run systematic experiments** comparing different agent configurations\n",
    "5. **Iterate continuously** based on evaluation results\n",
    "\n",
    "### üåü The Impact\n",
    "\n",
    "This scientific approach to agent development enables:\n",
    "- **Faster debugging** of agent failures\n",
    "- **Evidence-based improvements** rather than guesswork\n",
    "- **Scalable quality assurance** as your system grows\n",
    "- **Measurable progress** toward your quality goals\n",
    "\n",
    "---\n",
    "\n",
    "*This tutorial was inspired by the **Arize Observe 2025** presentation on scientific approaches to AI agent evaluation. For more resources on agent observability and evaluation, visit [docs.arize.com/phoenix](https://docs.arize.com/phoenix/).*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
