{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Quickstart: Datasets and Experiments in Deno</h1>\n",
    "\n",
    "Phoenix helps you run experiments over your AI and LLM applications to evaluate and iteratively improve their performance. This quickstart shows you how to get up and running quickly with the JavaScript SDK in a Deno environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createClient } from \"npm:@arizeai/phoenix-client\";\n",
    "import {runExperiment, asEvaluator} from \"npm:@arizeai/phoenix-client/experiments\";\n",
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createClient, runExperiment, asEvaluator } from \"npm:@arizeai/phoenix-client\";\n",
>>>>>>> d355a8609 (add deno)
    "import { OpenAI } from \"npm:openai\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 6,
>>>>>>> d355a8609 (add deno)
   "metadata": {},
   "outputs": [],
   "source": [
    "const openaiApiKey = prompt(\"Enter your OpenAI API key:\");\n",
    "\n",
    "if (!openaiApiKey) {\n",
    "  console.error('Please enter your OpenAI API key');\n",
    "  Deno.exit(1);\n",
    "}\n",
    "\n",
    "const openai = new OpenAI({\n",
    "  apiKey: openaiApiKey,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Phoenix client."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 7,
>>>>>>> d355a8609 (add deno)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix client initialized. Access Phoenix UI at http://localhost:6006\n"
     ]
    }
   ],
   "source": [
    "const client = createClient();\n",
    "console.log('Phoenix client initialized. Access Phoenix UI at http://localhost:6006');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset\n",
    "\n",
    "Let's create examples for our dataset."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 8,
>>>>>>> d355a8609 (add deno)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset examples...\n",
      "Created 3 example(s)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Example dataset entries\n",
       "\n",
       "```json\n",
       "[\n",
       "  {\n",
       "    \"id\": \"example-1\",\n",
<<<<<<< HEAD
       "    \"updatedAt\": \"2025-05-20T19:28:40.675Z\",\n",
=======
       "    \"updatedAt\": \"2025-05-20T16:39:12.223Z\",\n",
>>>>>>> d355a8609 (add deno)
       "    \"input\": {\n",
       "      \"question\": \"What is Paul Graham known for?\"\n",
       "    },\n",
       "    \"output\": {\n",
       "      \"answer\": \"Co-founding Y Combinator and writing on startups and techology.\"\n",
       "    },\n",
       "    \"metadata\": {\n",
       "      \"topic\": \"tech\"\n",
       "    }\n",
       "  },\n",
       "  {\n",
       "    \"id\": \"example-2\",\n",
<<<<<<< HEAD
       "    \"updatedAt\": \"2025-05-20T19:28:40.675Z\",\n",
=======
       "    \"updatedAt\": \"2025-05-20T16:39:12.223Z\",\n",
>>>>>>> d355a8609 (add deno)
       "    \"input\": {\n",
       "      \"question\": \"What companies did Elon Musk found?\"\n",
       "    },\n",
       "    \"output\": {\n",
       "      \"answer\": \"Tesla, SpaceX, Neuralink, The Boring Company, and co-founded PayPal.\"\n",
       "    },\n",
       "    \"metadata\": {\n",
       "      \"topic\": \"entrepreneurs\"\n",
       "    }\n",
       "  },\n",
       "  {\n",
       "    \"id\": \"example-3\",\n",
<<<<<<< HEAD
       "    \"updatedAt\": \"2025-05-20T19:28:40.675Z\",\n",
=======
       "    \"updatedAt\": \"2025-05-20T16:39:12.223Z\",\n",
>>>>>>> d355a8609 (add deno)
       "    \"input\": {\n",
       "      \"question\": \"What is Moore's Law?\"\n",
       "    },\n",
       "    \"output\": {\n",
       "      \"answer\": \"The observation that the number of transistors in a dense integrated circuit doubles about every two years.\"\n",
       "    },\n",
       "    \"metadata\": {\n",
       "      \"topic\": \"computing\"\n",
       "    }\n",
       "  }\n",
       "]\n",
       "```\n"
      ]
     },
<<<<<<< HEAD
     "execution_count": 18,
=======
     "execution_count": 8,
>>>>>>> d355a8609 (add deno)
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "console.log('Creating dataset examples...');\n",
    "\n",
    "// Create examples directly as an array\n",
    "const examples = [\n",
    "  {\n",
    "    id: `example-1`,\n",
    "    updatedAt: new Date(),\n",
    "    input: { question: \"What is Paul Graham known for?\" },\n",
    "    output: { answer: \"Co-founding Y Combinator and writing on startups and techology.\" },\n",
    "    metadata: { topic: \"tech\" }\n",
    "  },\n",
    "  {\n",
    "    id: `example-2`,\n",
    "    updatedAt: new Date(),\n",
    "    input: { question: \"What companies did Elon Musk found?\" },\n",
    "    output: { answer: \"Tesla, SpaceX, Neuralink, The Boring Company, and co-founded PayPal.\" },\n",
    "    metadata: { topic: \"entrepreneurs\" }\n",
    "  },\n",
    "  {\n",
    "    id: `example-3`,\n",
    "    updatedAt: new Date(),\n",
    "    input: { question: \"What is Moore's Law?\" },\n",
    "    output: { answer: \"The observation that the number of transistors in a dense integrated circuit doubles about every two years.\" },\n",
    "    metadata: { topic: \"computing\" }\n",
    "  }\n",
    "];\n",
    "\n",
    "console.log(`Created ${examples.length} example(s)`);\n",
    "\n",
    "await Deno.jupyter.md`\n",
    "### Example dataset entries\n",
    "\n",
    "\\`\\`\\`json\n",
    "${JSON.stringify(examples, null, 2)}\n",
    "\\`\\`\\`\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Task\n",
    "\n",
    "Define the task function that will be evaluated."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 9,
>>>>>>> d355a8609 (add deno)
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Test task result: Paul Graham is known for co-founding Y Combinator and his essays on technology and startups.\n"
=======
      "Test task result: Paul Graham is known for co-founding Y Combinator and his essays on startups and entrepreneurship.\n"
>>>>>>> d355a8609 (add deno)
     ]
    }
   ],
   "source": [
    "const taskPromptTemplate = \"Answer in a few words: {question}\";\n",
    "\n",
    "const task = async (example) => {\n",
    "  // Safely access question with a type assertion\n",
    "  const question = example.input.question || \"No question provided\";\n",
    "  const messageContent = taskPromptTemplate.replace('{question}', question);\n",
    "  \n",
    "  const response = await openai.chat.completions.create({\n",
    "    model: \"gpt-4o\",\n",
    "    messages: [{ role: \"user\", content: messageContent }]\n",
    "  });\n",
    "  \n",
    "  return response.choices[0]?.message?.content || \"\";\n",
    "};\n",
    "\n",
    "// Test the task on one example\n",
    "const testResult = await task(examples[0]);\n",
    "console.log(\"Test task result:\", testResult);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Evaluators\n",
    "\n",
    "Let's define evaluators for our experiment."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'contains_keyword' evaluator\n"
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Relative import path \"@arizeai/openinference-semantic-conventions\" not prefixed with / or ./ or ../\n  \u001b[0m\u001b[36mhint:\u001b[0m If you want to use a JSR or npm package, try running `deno add jsr:@arizeai/openinference-semantic-conventions` or `deno add npm:@arizeai/openinference-semantic-conventions`\n    at \u001b[0m\u001b[36mfile:///Users/donnyli/Desktop/phoenix/js/packages/phoenix-client/src/experiments/runExperiment.ts\u001b[0m:\u001b[0m\u001b[33m25\u001b[0m:\u001b[0m\u001b[33m8\u001b[0m",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "TypeError: Relative import path \"@arizeai/openinference-semantic-conventions\" not prefixed with / or ./ or ../",
      "  \u001b[0m\u001b[36mhint:\u001b[0m If you want to use a JSR or npm package, try running `deno add jsr:@arizeai/openinference-semantic-conventions` or `deno add npm:@arizeai/openinference-semantic-conventions`",
      "    at \u001b[0m\u001b[36mfile:///Users/donnyli/Desktop/phoenix/js/packages/phoenix-client/src/experiments/runExperiment.ts\u001b[0m:\u001b[0m\u001b[33m25\u001b[0m:\u001b[0m\u001b[33m8\u001b[0m",
      "    at async <anonymous>:1:46"
>>>>>>> d355a8609 (add deno)
     ]
    }
   ],
   "source": [
    "// 1. Code-based evaluator that checks if response contains specific keywords\n",
    "const containsKeyword = asEvaluator({\n",
    "  name: \"contains_keyword\",\n",
    "  kind: \"CODE\",\n",
    "  evaluate: async ({ output }) => {\n",
    "    const keywords = [\"Y Combinator\", \"YC\"];\n",
    "    const outputStr = String(output).toLowerCase();\n",
    "    const contains = keywords.some(keyword => \n",
    "      outputStr.toLowerCase().includes(keyword.toLowerCase())\n",
    "    );\n",
    "    \n",
    "    return {\n",
    "      score: contains ? 1.0 : 0.0,\n",
    "      label: contains ? \"contains_keyword\" : \"missing_keyword\",\n",
    "      metadata: { keywords },\n",
    "      explanation: contains ? \n",
    "        `Output contains one of the keywords: ${keywords.join(\", \")}` : \n",
    "        `Output does not contain any of the keywords: ${keywords.join(\", \")}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'contains_keyword' evaluator\");"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'conciseness' evaluator\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d355a8609 (add deno)
   "source": [
    "// 2. LLM-based evaluator for conciseness\n",
    "const conciseness = asEvaluator({\n",
    "  name: \"conciseness\",\n",
    "  kind: \"LLM\", \n",
    "  evaluate: async ({ output }) => {\n",
    "    const prompt = `\n",
    "      Rate the following text on a scale of 0.0 to 1.0 for conciseness (where 1.0 is perfectly concise).\n",
    "      \n",
    "      TEXT: ${output}\n",
    "      \n",
    "      Return only a number between 0.0 and 1.0.\n",
    "    `;\n",
    "    \n",
    "    const response = await openai.chat.completions.create({\n",
    "      model: \"gpt-4o\",\n",
    "      messages: [{ role: \"user\", content: prompt }]\n",
    "    });\n",
    "    \n",
    "    const scoreText = response.choices[0]?.message?.content?.trim() || \"0\";\n",
    "    const score = parseFloat(scoreText);\n",
    "    \n",
    "    return {\n",
    "      score: isNaN(score) ? 0.5 : score,\n",
    "      label: score > 0.7 ? \"concise\" : \"verbose\",\n",
    "      metadata: {},\n",
    "      explanation: `Conciseness score: ${score}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'conciseness' evaluator\");"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'jaccard_similarity' evaluator\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d355a8609 (add deno)
   "source": [
    "// 3. Custom Jaccard similarity evaluator\n",
    "const jaccardSimilarity = asEvaluator({\n",
    "  name: \"jaccard_similarity\",\n",
    "  kind: \"CODE\",\n",
    "  evaluate: async ({ output, expected }) => {\n",
    "    const actualWords = new Set(String(output).toLowerCase().split(\" \"));\n",
    "    const expectedAnswer = expected?.answer || \"\";\n",
    "    const expectedWords = new Set(expectedAnswer.toLowerCase().split(\" \"));\n",
    "    \n",
    "    const wordsInCommon = new Set(\n",
    "      [...actualWords].filter(word => expectedWords.has(word))\n",
    "    );\n",
    "    \n",
    "    const allWords = new Set([...actualWords, ...expectedWords]);\n",
    "    const score = wordsInCommon.size / allWords.size;\n",
    "    \n",
    "    return {\n",
    "      score,\n",
    "      label: score > 0.5 ? \"similar\" : \"dissimilar\",\n",
    "      metadata: { \n",
    "        actualWordsCount: actualWords.size,\n",
    "        expectedWordsCount: expectedWords.size,\n",
    "        commonWordsCount: wordsInCommon.size,\n",
    "        allWordsCount: allWords.size\n",
    "      },\n",
    "      explanation: `Jaccard similarity: ${score}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'jaccard_similarity' evaluator\");"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'accuracy' evaluator\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d355a8609 (add deno)
   "source": [
    "// 4. LLM-based accuracy evaluator\n",
    "const accuracy = asEvaluator({\n",
    "  name: \"accuracy\",\n",
    "  kind: \"LLM\",\n",
    "  evaluate: async ({ input, output, expected }) => {\n",
    "    // Safely access question and answer with type assertions and fallbacks\n",
    "    const question = input.question || \"No question provided\";\n",
    "    const referenceAnswer = expected?.answer || \"No reference answer provided\";\n",
    "    \n",
    "    const evalPromptTemplate = `\n",
    "      Given the QUESTION and REFERENCE_ANSWER, determine whether the ANSWER is accurate.\n",
    "      Output only a single word (accurate or inaccurate).\n",
    "      \n",
    "      QUESTION: {question}\n",
    "      \n",
    "      REFERENCE_ANSWER: {reference_answer}\n",
    "      \n",
    "      ANSWER: {answer}\n",
    "      \n",
    "      ACCURACY (accurate / inaccurate):\n",
    "    `;\n",
    "    \n",
    "    const messageContent = evalPromptTemplate\n",
    "      .replace('{question}', question)\n",
    "      .replace('{reference_answer}', referenceAnswer)\n",
    "      .replace('{answer}', String(output));\n",
    "    \n",
    "    const response = await openai.chat.completions.create({\n",
    "      model: \"gpt-4o\",\n",
    "      messages: [{ role: \"user\", content: messageContent }]\n",
    "    });\n",
    "    \n",
    "    const responseContent = response.choices[0]?.message?.content?.toLowerCase().trim() || \"\";\n",
    "    const isAccurate = responseContent === \"accurate\";\n",
    "    \n",
    "    return {\n",
    "      score: isAccurate ? 1.0 : 0.0,\n",
    "      label: isAccurate ? \"accurate\" : \"inaccurate\",\n",
    "      metadata: {},\n",
    "      explanation: `LLM determined the answer is ${isAccurate ? \"accurate\" : \"inaccurate\"}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'accuracy' evaluator\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiment\n",
    "\n",
    "Now let's run the experiment with our defined evaluators."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running initial experiment...\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "TODO: implement dataset creation from examples",
     "output_type": "error",
     "traceback": [
      "Stack trace:",
      "Error: TODO: implement dataset creation from examples",
      "    at getDatasetBySelector (file:///Users/donnyli/Library/Caches/deno/npm/registry.npmjs.org/@arizeai/phoenix-client/1.3.0/dist/esm/utils/getDatasetBySelector.js:33:15)",
      "    at runExperiment (file:///Users/donnyli/Library/Caches/deno/npm/registry.npmjs.org/@arizeai/phoenix-client/1.3.0/dist/esm/experiments/runExperiment.js:48:27)",
      "    at <anonymous>:3:26"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> d355a8609 (add deno)
   "source": [
    "console.log('Running initial experiment...');\n",
    "\n",
    "// Pass dataset directly as the array of examples\n",
    "const experiment = await runExperiment({\n",
    "  client,\n",
    "  experimentName: \"initial-experiment\",\n",
    "  dataset: examples,\n",
    "  task,\n",
    "  evaluators: [jaccardSimilarity, accuracy],\n",
    "  logger: console,\n",
    "});\n",
    "\n",
    "console.log('Initial experiment completed with ID:', experiment.id);\n",
    "\n",
    "await Deno.jupyter.md`\n",
    "### Initial experiment results\n",
    "\n",
    "Experiment ID: \\`${experiment.id}\\`\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Additional Evaluators\n",
    "\n",
    "Let's run more evaluators on the same experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log('Running additional evaluators...');\n",
    "\n",
    "const updatedExperiment = await runExperiment({\n",
    "  client,\n",
    "  experimentName: experiment.id, // Use the same experiment ID\n",
    "  dataset: examples, // Use the array of examples\n",
    "  task: async () => \"\", // No-op task since we're just evaluating\n",
    "  evaluators: [containsKeyword, conciseness],\n",
    "  logger: console\n",
    "});\n",
    "\n",
    "console.log('Additional evaluations completed');\n",
    "console.log('Experiment ID:', updatedExperiment.id);\n",
    "\n",
    "await Deno.jupyter.md`\n",
    "### Final experiment results\n",
    "\n",
    "The experiment has been updated with additional evaluators. View the complete results in the Phoenix UI:\n",
    "\n",
    "http://localhost:6006\n",
    "\n",
    "Experiment ID: \\`${updatedExperiment.id}\\`\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully:\n",
    "1. Created a dataset with examples\n",
    "2. Defined a task using the OpenAI API\n",
    "3. Created multiple evaluators using both code-based and LLM-based approaches\n",
    "4. Run an experiment and evaluated the results\n",
    "5. Added additional evaluators to the experiment\n",
    "\n",
    "You can now explore the results in the Phoenix UI and iterate on your experiments to improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
