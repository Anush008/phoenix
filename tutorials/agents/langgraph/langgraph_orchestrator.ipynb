{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Buqnnxn0L7"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTuMwH8Qg3kn"
   },
   "source": [
    "# LangGraph Agents: Orchestrator–Worker Pattern\n",
    "\n",
    "In this tutorial, we’ll build a multi-agent system using LangGraph's **Orchestrator–Worker pattern**, ideal for dynamically decomposing a task into subtasks, assigning them to specialized LLM agents, and synthesizing their responses.\n",
    "\n",
    "This pattern is particularly well-suited when the structure of subtasks is unknown ahead of time—such as when writing modular code, creating multi-section reports, or conducting research. The **orchestrator** plans and delegates, while the **workers** each complete their assigned section.\n",
    "\n",
    "We’ll also use **Phoenix** to trace and debug the orchestration process. With Phoenix, you can visually inspect which tasks the orchestrator generated, how each worker handled its section, and how the final output was assembled.\n",
    "\n",
    "By the end of this notebook, you’ll learn how to:\n",
    "- Use structured outputs to plan subtasks dynamically.\n",
    "- Assign subtasks to LLM workers via LangGraph's `Send` API.\n",
    "- Collect and synthesize multi-step LLM outputs.\n",
    "- Trace and visualize orchestration using Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain langchain_community arize-phoenix arize-phoenix-otel openinference-instrumentation-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔑 Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQuJO1HhQmH"
   },
   "source": [
    "# Configure Phoenix Tracing\n",
    "\n",
    "Make sure you go to https://app.phoenix.arize.com/ and generate an API key. This will allow you to trace your Langgraph application with Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PHOENIX_API_KEY\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"🔑 Enter your Phoenix API key: \")\n",
    "\n",
    "if \"PHOENIX_COLLECTOR_ENDPOINT\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = getpass(\"🔑 Enter your Phoenix Collector Endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(project_name=\"Orchestrator\", auto_instrument=False)\n",
    "\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dATeDmFbhcWY"
   },
   "source": [
    "Orchestrator‑Workers • Research‑Paper Generator\n",
    "----------------------------------------------\n",
    "The orchestrator plans research‑paper *subsections* (abstract, background …),\n",
    "spawns one worker per subsection, then stitches everything into a full draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piT0RAtclEW-"
   },
   "source": [
    "# Step 1: Defining the Planning Schema\n",
    "To begin, we define a structured output schema using Pydantic. This schema ensures that the LLM returns well-formatted, predictable output when tasked with planning the structure of a research paper.\n",
    "\n",
    "We create two models:\n",
    "\n",
    "Subsection: Represents a single unit of the paper, including its name and a brief description of what it should cover.\n",
    "\n",
    "Subsections: A wrapper that holds a list of these units.\n",
    "\n",
    "By using these models with LangGraph’s with_structured_output feature, we enforce that the orchestrator LLM returns an organized plan — rather than freeform text — that downstream nodes (worker LLMs) can reliably use.\n",
    "\n",
    "This schema acts as the blueprint for the rest of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    name: str = Field(description=\"Name for this subsection of the research paper.\")\n",
    "    description: str = Field(\n",
    "        description=\"Concise description of the general subjects to be covered in this subsection.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Subsections(BaseModel):\n",
    "    Subsections: List[Subsection] = Field(description=\"All subsections of the research paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcIUeErDlTFb"
   },
   "source": [
    "# Step 2: Set Up LLM and Tools\n",
    "We initialize gpt-3.5-turbo as our base LLM and bind it to the Subsections schema to create the orchestrator. We also load a DuckDuckGo search tool to allow worker agents to enrich sections with live web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILIY_API_KEY = getpass(\"Tavily API Key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILIY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "orchestrator_llm = llm.with_structured_output(Subsections)\n",
    "\n",
    "search = TavilySearchResults(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1R_tZc_laar"
   },
   "source": [
    "# Step 3: Define Graph State\n",
    "We define two state schemas:\n",
    "\n",
    "State holds the overall research paper workflow, including the topic, planned subsections, completed text, and final output.\n",
    "\n",
    "WorkerState captures the task assigned to each worker — a single subsection — and where their contributions are accumulated.\n",
    "\n",
    "This shared state structure lets LangGraph coordinate work between the orchestrator and its worker agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    subsections: List[Subsection]\n",
    "    completed_subsections: Annotated[List[str], operator.add]\n",
    "    final_paper: str\n",
    "    search_results: str\n",
    "\n",
    "\n",
    "class WorkerState(TypedDict):\n",
    "    subsection: Annotated[Subsection, lambda x, y: y]\n",
    "    completed_subsections: Annotated[List[str], operator.add]\n",
    "    search_results: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ID2JeffkliCe"
   },
   "source": [
    "# Step 4: Define Nodes\n",
    "We define three core nodes in the graph:\n",
    "\n",
    "orchestrator: Dynamically plans the structure of the paper by generating a list of subsections using structured output.\n",
    "\n",
    "subsection_writer: Acts as a worker that writes one full subsection in academic Markdown, using the provided description and scope.\n",
    "\n",
    "synthesiser: Merges all completed subsections into a single cohesive draft, separating sections with visual dividers.\n",
    "\n",
    "Each node contributes to a modular, scalable paper-writing pipeline — and with Phoenix tracing, you can inspect every generation step in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State):\n",
    "    \"\"\"Plan the research‑paper subsections dynamically.\"\"\"\n",
    "    plan = orchestrator_llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a detailed subsection plan for a research paper.\"),\n",
    "            HumanMessage(content=f\"Paper topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"subsections\": plan.Subsections}\n",
    "\n",
    "\n",
    "def subsection_writer(state: WorkerState):\n",
    "    sub = state[\"subsection\"]\n",
    "    search_info = state.get(\"search_results\", \"\")\n",
    "\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You're writing a research-paper subsection using the following web search result as background and also your own knowledge.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Subsection: {sub.name}\\n\"\n",
    "                    f\"Description: {sub.description}\\n\"\n",
    "                    f\"Shared Search Results:\\n{search_info}\\n\\n\"\n",
    "                    \"Now write the section.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"completed_subsections\": [response.content]}\n",
    "\n",
    "\n",
    "def synthesiser(state: State):\n",
    "    \"\"\"Concatenate all finished subsections into the final paper draft.\"\"\"\n",
    "    full_paper = \"\\n\\n---\\n\\n\".join(state[\"completed_subsections\"])\n",
    "    return {\"final_paper\": full_paper}\n",
    "\n",
    "\n",
    "def search_tool(state: State):\n",
    "    query = f\"{state['topic']} research summary\"\n",
    "    search_results = search.invoke(query)\n",
    "    return {\"search_results\": search_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNqMCcNol8U2"
   },
   "source": [
    "# Step 5: Assign Workers Dynamically\n",
    "This function uses LangGraph’s Send API to launch a separate subsection_writer worker for each planned subsection. By dynamically spawning one worker per section, the system scales flexibly based on the topic’s complexity.\n",
    "\n",
    "This approach is ideal for research paper generation, where the number of sections is not known ahead of time — and Phoenix helps trace the output from each worker node independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_workers(state: State):\n",
    "    \"\"\"Launch one subsection_writer per planned subsection (after shared search).\"\"\"\n",
    "    return [\n",
    "        Send(\"subsection_writer\", {\"subsection\": s, \"search_results\": state[\"search_results\"]})\n",
    "        for s in state[\"subsections\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ijfe_y9mFDO"
   },
   "source": [
    "# Step 6: Construct the LangGraph Workflow\n",
    "Here, we build the full LangGraph pipeline using a StateGraph. The workflow begins with the orchestrator node (to plan subsections), dynamically routes work to subsection_writer nodes (via assign_workers), and then aggregates all outputs in the synthesiser node.\n",
    "\n",
    "LangGraph’s conditional edges and Send API enable scalable parallelism — and with Phoenix tracing enabled, you can view how each section is created, tracked, and stitched together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"orchestrator\", orchestrator)\n",
    "builder.add_node(\"search_tool\", search_tool)\n",
    "builder.add_node(\"subsection_writer\", subsection_writer)\n",
    "builder.add_node(\"synthesiser\", synthesiser)\n",
    "\n",
    "builder.add_edge(START, \"orchestrator\")\n",
    "builder.add_edge(\"orchestrator\", \"search_tool\")\n",
    "builder.add_conditional_edges(\"search_tool\", assign_workers, [\"subsection_writer\"])\n",
    "builder.add_edge(\"subsection_writer\", \"synthesiser\")\n",
    "builder.add_edge(\"synthesiser\", END)\n",
    "\n",
    "\n",
    "research_paper_workflow = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9sTPp-tmWIB"
   },
   "source": [
    "# Step 7: Run the Research Paper Generator\n",
    "We now invoke the compiled LangGraph with a sample topic: “Scaling Laws for Large Language Models.” The orchestrator plans the outline, each worker drafts a subsection in parallel, and the synthesizer assembles the full paper.\n",
    "\n",
    "With Phoenix integrated, every step is traced — from section planning to writing and synthesis — giving you full visibility into the execution flow and helping debug or refine outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topics = [\n",
    "    \"How do scaling laws impact the performance of large language models?\",\n",
    "    \"What are the key challenges in training very large transformer models?\",\n",
    "    \"How much data is needed to train a performant LLM?\",\n",
    "    \"Explain the relationship between model size and accuracy in language models.\",\n",
    "    \"Why are modern language models undertrained, and how can we fix it?\",\n",
    "    \"What is compute-optimal training for LLMs?\",\n",
    "    \"Compare different scaling strategies for training foundation models.\",\n",
    "    \"How do researchers determine the best size for a transformer model?\",\n",
    "    \"What are the trade-offs between training time and model performance?\",\n",
    "    \"Summarize recent findings on training efficiency for large-scale language models.\",\n",
    "]\n",
    "\n",
    "for topic in research_topics:\n",
    "    state = research_paper_workflow.invoke({\"topic\": topic})\n",
    "\n",
    "print(\"===== RESEARCH PAPER DRAFT =====\\n\")\n",
    "Markdown(state[\"final_paper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkfIm0uSmdpa"
   },
   "source": [
    "# Step 8: Check out your traces in Phoenix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlo8OLX_fdgM"
   },
   "source": [
    "# Let's add some Evaluations (Evals)\n",
    "\n",
    "In this section we will evaluate Agent Trajectory. \n",
    "\n",
    "See https://arize.com/docs/ax/evaluate/agent-trajectory-evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "df = px.Client().get_spans_dataframe(project_name=\"Orchestrator\", timeout=None)\n",
    "llm_spans = df[df[\"span_kind\"] == \"LLM\"]\n",
    "root_ids = df[df[\"parent_id\"].isna()][\"context.trace_id\"].unique()\n",
    "llm_spans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAJECTORY_ACCURACY_PROMPT_WITHOUT_REFERENCE = \"\"\"\n",
    "You are a helpful AI bot that checks whether an AI agent's internal trajectory is accurate and effective.\n",
    "\n",
    "You will be given:\n",
    "1. The agent's actual trajectory of tool calls\n",
    "2. You will be given input data from a user that the agent used to make a decision\n",
    "3. You will be given a tool call definition, what the agent used to make the tool call\n",
    "\n",
    "An accurate trajectory:\n",
    "- Progresses logically from step to step\n",
    "- Follows the golden trajectory where reasonable\n",
    "- Shows a clear path toward completing a goal\n",
    "- Is reasonably efficient (doesn't take unnecessary detours)\n",
    "\n",
    "##\n",
    "\n",
    "Actual Trajectory:\n",
    "{tool_calls}\n",
    "\n",
    "Use Inputs:\n",
    "{attributes.input.value}\n",
    "\n",
    "Tool Definitions:\n",
    "{attributes.llm.tools}\n",
    "\n",
    "##\n",
    "\n",
    "Your response must be a single string, either `correct` or `incorrect`, and must not include any additional text.\n",
    "\n",
    "- Respond with `correct` if the agent's trajectory adheres to the rubric and accomplishes the task effectively.\n",
    "- Respond with `incorrect` if the trajectory is confusing, misaligned with the goal, inefficient, or does not accomplish the task.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def filter_spans_by_trace_criteria(\n",
    "    df: pd.DataFrame,\n",
    "    trace_filters: Dict[str, Dict[str, Any]],\n",
    "    span_filters: Dict[str, Dict[str, Any]],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter spans based on trace-level and span-level criteria.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with trace data\n",
    "        trace_filters: Dictionary of column names and filtering criteria for traces\n",
    "                      Format: {\"column_name\": {\"operator\": value}}\n",
    "                      Supported operators: \">=\", \"<=\", \"==\", \"!=\", \"contains\", \"notna\", \"isna\"\n",
    "        span_filters: Dictionary of column names and filtering criteria for spans\n",
    "                     Format: {\"column_name\": {\"operator\": value}}\n",
    "                     Same supported operators as trace_filters\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with filtered spans from traces that match trace_filters\n",
    "    \"\"\"\n",
    "    all_trace_ids = set(df[\"context.trace_id\"].unique())\n",
    "    print(f\"Total traces: {len(all_trace_ids)}\")\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    traces_df = df_copy.copy()\n",
    "    for column, criteria in trace_filters.items():\n",
    "        if column not in traces_df.columns:\n",
    "            print(f\"Warning: Column '{column}' not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        for sign, value in criteria.items():\n",
    "            if sign == \">=\":\n",
    "                matching_spans = traces_df[traces_df[column] >= value]\n",
    "            elif sign == \"<=\":\n",
    "                matching_spans = traces_df[traces_df[column] <= value]\n",
    "            elif sign == \"==\":\n",
    "                matching_spans = traces_df[traces_df[column] == value]\n",
    "            elif sign == \"!=\":\n",
    "                matching_spans = traces_df[traces_df[column] != value]\n",
    "            elif sign == \"contains\":\n",
    "                matching_spans = traces_df[\n",
    "                    traces_df[column].str.contains(value, case=False, na=False)\n",
    "                ]\n",
    "            elif sign == \"isna\":\n",
    "                matching_spans = traces_df[traces_df[column].isna()]\n",
    "            elif sign == \"notna\":\n",
    "                matching_spans = traces_df[traces_df[column].notna()]\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported operator '{sign}' - skipping\")\n",
    "                continue\n",
    "\n",
    "            traces_df = matching_spans\n",
    "\n",
    "    matching_trace_ids = set(traces_df[\"context.trace_id\"].unique())\n",
    "    print(f\"Found {len(matching_trace_ids)} traces matching trace criteria\")\n",
    "\n",
    "    if not matching_trace_ids:\n",
    "        print(\"No matching traces found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    result_df = df[df[\"context.trace_id\"].isin(matching_trace_ids)].copy()\n",
    "\n",
    "    for column, criteria in span_filters.items():\n",
    "        if column not in result_df.columns:\n",
    "            print(f\"Warning: Column '{column}' not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        for sign, value in criteria.items():\n",
    "            if sign == \">=\":\n",
    "                result_df = result_df[result_df[column] >= value]\n",
    "            elif sign == \"<=\":\n",
    "                result_df = result_df[result_df[column] <= value]\n",
    "            elif sign == \"==\":\n",
    "                result_df = result_df[result_df[column] == value]\n",
    "            elif sign == \"!=\":\n",
    "                result_df = result_df[result_df[column] != value]\n",
    "            elif sign == \"contains\":\n",
    "                result_df = result_df[result_df[column].str.contains(value, case=False, na=False)]\n",
    "            elif sign == \"isna\":\n",
    "                result_df = result_df[result_df[column].isna()]\n",
    "            elif sign == \"notna\":\n",
    "                result_df = result_df[result_df[column].notna()]\n",
    "            else:\n",
    "                print(f\"Warning: Unsupported operator '{sign}' - skipping\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Final result: {len(result_df)} spans from {len(matching_trace_ids)} traces\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def extract_tool_calls(output_messages):\n",
    "    if not output_messages:\n",
    "        return []\n",
    "\n",
    "    tool_calls = []\n",
    "    for message in output_messages:\n",
    "        if \"message.tool_calls\" in message:\n",
    "            for tool_call in message[\"message.tool_calls\"]:\n",
    "                tool_calls.append({\"name\": tool_call[\"tool_call.function.name\"]})\n",
    "    return tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "eval_traces = filter_spans_by_trace_criteria(\n",
    "    df=df,\n",
    "    trace_filters={\"name\": {\"contains\": \"agent\"}},\n",
    "    span_filters={\"attributes.openinference.span.kind\": {\"==\": \"LLM\"}},\n",
    ")\n",
    "\n",
    "eval_traces.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_traces[\"tool_calls\"] = eval_traces[\"attributes.llm.output_messages\"].apply(extract_tool_calls)\n",
    "eval_traces.head()\n",
    "full_eval_spans = eval_traces[eval_traces[\"attributes.llm.tools\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "from phoenix.trace import suppress_tracing\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model = OpenAIModel(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "rails = [\"correct\", \"incorrect\"]\n",
    "\n",
    "with suppress_tracing():\n",
    "    eval_results = llm_classify(\n",
    "        dataframe=full_eval_spans,\n",
    "        template=TRAJECTORY_ACCURACY_PROMPT_WITHOUT_REFERENCE,\n",
    "        model=model,\n",
    "        rails=rails,\n",
    "        provide_explanation=True,\n",
    "        verbose=False,\n",
    "        concurrency=20,\n",
    "    )\n",
    "\n",
    "eval_results[\"score\"] = eval_results[\"label\"].apply(lambda x: 1 if x == \"correct\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_df = pd.merge(full_eval_spans, eval_results, left_index=True, right_index=True)\n",
    "\n",
    "merged_df.rename(columns={\"context.trace_id\": \"context.span_id\"}, inplace=True)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(\n",
    "        dataframe=merged_df,\n",
    "        eval_name=\"Agent Trajectory Accuracy\",\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
