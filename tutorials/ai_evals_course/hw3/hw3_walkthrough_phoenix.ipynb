{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: LLM-as-Judge - Phoenix Walkthrough "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the reference implementation for Homework 3, and discusses additional topics.  This notebook was reviewed in Priyan's HW review video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Phoenix\n",
    "\n",
    "First, make sure you have a Phoenix account set up. (If you have already done this in a previous HW assignment, you are good to go.)\n",
    "\n",
    "**Sign up for Phoenix**:\n",
    "\n",
    "1. Sign up for an Arize Phoenix account at https://app.phoenix.arize.com/login\n",
    "2. Click Create Space, then follow the prompts to create and launch your space.\n",
    "\n",
    "**Install packages**:\n",
    "\n",
    "```pip install arize-phoenix-otel```\n",
    "\n",
    "**Set your Phoenix endpoint and API Key**:\n",
    "\n",
    "From your new Phoenix Space\n",
    "\n",
    "1. Create your API key from the Settings page\n",
    "2. Copy your Hostname from the Settings page\n",
    "3. In your terminal, set your endpoint and API key within your current environment:\n",
    "\n",
    "```\n",
    "export PHOENIX_API_KEY=\"ADD YOUR PHOENIX API KEY\"\n",
    "export PHOENIX_API_KEY=\"ADD YOUR PHOENIX HOSTNAME\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The homework instructions for this homework assignment is in `homeworks/hw3/README.md`.  The content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the start of the process.  It starts with queries that map to a dietary restriction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 data/'dietary_queries.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traces are generated by `generate_traces.py`, which runs those queries though the model.  Some key notes:\n",
    "\n",
    "The `generate_traces.py` script calls `get_agent_response` **from the application code**.  This is ideal to minimize differences between experiments and production.\n",
    "\n",
    "Phoenix allows you to add just a few lines of code and fully trace your application! See lines 15-18 in `generate_traces.py` which set up Phoenix tracing, and the `generate_trace_with_id` function that calls get_agent_response and sets custom trace attributes.\n",
    "\n",
    "***You can view your traces in Phoenix, within the space you created!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use AI to create these, but you **must** look at them carefully.  For example, consider this row:\n",
    "\n",
    "> 43,Comfort food that won't make me feel guilty,vegetarian\n",
    "\n",
    "Given the `query`, how could your application know that it should be a vegetarian dish.  This is a bad query as the query does not provide any way for the LLM to know it's supposed to be generating a vegetarian recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to label and create ground truth labels for traces.  The `label_data.py` script creates these ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 data/'labeled_traces.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a csv that has the `query` and `dietary_restriction` from the previous step.  But also the most critical pieces are:\n",
    "\n",
    "- `attributes.output.value`:  The model response to the query (should be a recipe).\n",
    "- `ground_truth_label`: Whether this recipe passed or failed to follow the dietary restriction\n",
    "- `ground_truth_explanation`: LLM explanation for why it gave the label it did\n",
    "- `ground_truth_confidence`: A rating of confidence the model gave for the label.  Useful for identifying likely model errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `label_data.py` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script creates these ground truth labels with LLMs.  Should you?  Probably not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to create an LLM as a judge.  By using an LLM to create the ground truth, you have essentially used an LLM as a judge to create the ground truth.  This can be ok if you use a much larger more powerful model for these, then are creating a much more scalable LLM as a judge with a smaller model.  HOWEVER....\n",
    "\n",
    "- This is not an excuse to not look at the data and label them\n",
    "- There are many pitfalls with this approach, that labeling manually with a domain expert completely avoids\n",
    "- When getting started this step is used as a way to look at less of the data.  But looking at the data is the singlemost high value activity\n",
    "\n",
    "So generally, you should just label your data manually.  Even if you use this LLM, you should then go in and label at least a sample of the LLM created ground truth labels to make sure the ground truth is high quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Phoenix eval functions were used to add evals to the Phoenix traces we built above.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Your Labeled Data\n",
    "\n",
    "In every data driven approach you should split your data in different sets.\n",
    "\n",
    "They may be called `train`, `validation`, and `test`.  Or `train`, `dev`, `test`.  But you need 3 sets.  This is true in machine learning as well as statistics.  They serve different purposes:\n",
    "\n",
    "- **train**: You can do anything with this and \"train\" you model on this data.  In this case training your model is using it to create few-shot examples for your prompt.  But you could do RAG against these, or use them for fine tuning or anything.  They are fair game for everything.\n",
    "- **validation**:  This is what you regularly measure against for development.  These cannot be used for RAG, or put in your prompt, or trained on.  But when you have a good solution you can iterate by testing how well it performs on the `validation` set.\n",
    "- **Test**: This is you ultimate protection to ensure your experiment results are going to translate to production and you can predict what the impact of your change will be.  Every time you measure against it and look at it you lose some of that protection.  So do so very sparingly! \n",
    "\n",
    "> This is to make sure you model can *generalize* beyond the specific things you have seen.  There are many words for overfitting in different contexts such as overfitting, p-hacking, data leakage, lookahead bias, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read labeled traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/labeled_traces.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random numbers and assign to appropriate categories based on those numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rand\"] = [np.random.random() for _ in range(len(df))]\n",
    "df[\"split\"] = np.where(\n",
    "    df[\"rand\"] < 0.15, \"train\", np.where(df[\"rand\"] < 0.55, \"validation\", \"test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df.split == \"train\"]\n",
    "dev_df = df[df.split == \"dev\"]\n",
    "test_df = df[df.split == \"test\"]\n",
    "train_df.shape, dev_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we do that we would save these splits so we have a consistent test set (we don't want to have test set values sometimes in the training set that we're looking at!)\n",
    "\n",
    "And look at how our categories are divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df.groupby([\"split\", \"label\"]).size().unstack(fill_value=0)\n",
    "stats[\"TTL\"] = stats.sum(axis=1)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common quersion is whether this is \"enough\"?  Let's think about this test set:\n",
    "\n",
    "There are 7 failures in the test set.  Do you think you'd feel better if your test set have 10 or 15 failures?  If so, label more!  Most problems can be avoided by looking at more data.\n",
    "\n",
    "One guideline I like is to consider if I had 20% less data.  If I did that do I think my experiment results could be skewed?  If so, I want to label more.  I could even do those experiments and see if they turn out differently.\n",
    "\n",
    "What we want is a `representative sample`.  There are many ways to estimate that statistically.  One way is to get a random sample of it many time, see if those smaller samples look like (are *representative*) of the whole set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Splitting Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `split_data.py` script uses a more advanced splitting approach called `stratified splitting`.\n",
    "\n",
    "Instead of making dev/test/train sets purely randomly, it ensures that each of the categories are proportionate in each set.  If 10% of the samples are `FAIL`, this it ensures that roughy 10% of the samples in each of the sets are `FAIL` and we don't end up with imbalanced based on random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you use stratified splitting?  Not neccesarily!  It's ok to start simple.  If random splitting is causing imbalanced labels in your sets there's 2 broad ways to fix it:\n",
    "\n",
    "1. **Best Way** Label more data until it's a non-issue with randomness!  This is usually the best way to go.  The more labeled samples you have the more unlikely randomness will cause \"wonkiness\"\n",
    "1. Make sure that they are balanced with stratified splitting.  This is really valuable when more labeling is extremely costly (either very hard to do, or classes or so imbalanced you'd have to label an insane amount to get the minority classes to have decent coverage).\n",
    "\n",
    "> NOTE:  This is an *another* example of where looking at your data manually can avoid complexity and a whole class of problems if you can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Your LLM-as-Judge Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `develop_judge.py` script creates a judge prompt with few-shot examples from the training set.\n",
    "\n",
    "This script chooses few-shot examples randomly to create a demonstration of what a prompt might look like.  During the process I recommend:\n",
    "\n",
    "1. Create a detailed criteria\n",
    "2. Look at training set and think about where applying that criteria might be hard or unclear\n",
    "3. Add the few-shot examples deliberately based on failure modes.\n",
    "4. Repeat\n",
    "\n",
    "For example for me, example 2 for example shows chicken in a vegetarian dietary restriction.  I suspect that this is a bit too obvious of a failure to add anything significantly helpful to the model.\n",
    "\n",
    "\n",
    "Guidelines for few-shot-examples for prompt:\n",
    "\n",
    "- Showing the model the full query and response is helpful.  If it is too large to do that you can play with it to try to make a more minimal reproduction of what you are trying to convey that is difficult to convey in the written instructions.\n",
    "- Include the correct answer\n",
    "- Include reasoning information for why the answer is correct that explains the nuance.  LLM generated reasoning can be a **starting** point, but this is critical and should be very carefully thought about and edited.\n",
    "\n",
    "Here, we use Phoenix eval functions to build our evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the judge prompt that was created\n",
    "with open(\"results/judge_prompt.txt\", \"r\") as f:\n",
    "    judge_prompt = f.read()\n",
    "\n",
    "# Display first 1000 characters to see the structure\n",
    "print(\"Judge Prompt Preview:\")\n",
    "print(\"=\" * 80)\n",
    "print(judge_prompt[:1000])\n",
    "print(\"\\n... [truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine & Validate Your Judge\n",
    "\n",
    "The judge was evaluated on dev set. Let's look at the performance metrics generated by `evaluate_judge.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load judge performance metrics\n",
    "with open(\"results/judge_performance.json\", \"r\") as f:\n",
    "    judge_perf = json.load(f)\n",
    "\n",
    "test_perf = judge_perf[\"test_set_performance\"]\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"TPR (True Positive Rate): {test_perf['true_positive_rate']:.3f}\")\n",
    "print(f\"TNR (True Negative Rate): {test_perf['true_negative_rate']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPR (True Positive Rate): 1.000**\n",
    "\n",
    "- What this means:  \n",
    "    - The judge correctly identifies 100% of responses that actually violate dietary restrictions\n",
    "    - No false negatives - you won't miss dangerous violations (e.g., serving meat to vegetarians)\n",
    "- Is this good?\n",
    "    - Probably not.  Is it reasonable to believe that the bot will **never** violate dietary restrictions?  Maybe, but more likely\n",
    "        - You test set is contaminated\n",
    "        - You eval code has a bug in it\n",
    "        - You don't have representative user queries\n",
    "        - Some other issue\n",
    "    - What if I checked all that and I'm certain it's really 100%?\n",
    "        - Maybe this task is just too easy?  If so, is it commercially viable?  Can someone else replicate it super easily?  Or is there other non-ai parts of the product that are providing enough value that it's ok that this isn't hard?\n",
    "        - You still don't have any information or signal for how to improve.  That should be worrying!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TNR (True Negative Rate): 0.750** \n",
    "- Your judge correctly identifies 75% of responses that properly follow dietary restrictions\n",
    "- 25% false positive rate - sometimes flags compliant responses as violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what's more important?  TPR or TNR?\n",
    "\n",
    "There are 2 possible failure modes in this task:\n",
    "\n",
    "1. The model creates a non-compliant recipe and thinks it is compliant (e.g. Supposed to be dairy-free but it has cheese in it)\n",
    "2. The model create a complient recipe and thinks it is not-compliant (e.g. Recipe is dairy free, but model thinks it has dairy in it and fails it).\n",
    "\n",
    "Break each down and consider the scenarious and impact of failing:\n",
    "\n",
    "#### The model creates a non-compliant recipe and thinks it is compliant \n",
    "\n",
    "Imagine...\n",
    "\n",
    "\n",
    "- A user asked for a dairy-free recipe and it has cheese in it:\n",
    "    - Is there harm to the user?  What if they are lactose intolerant?\n",
    "    - Does this effect churn/subscription likihood?  By a lot or a little?\n",
    "- A user asked for shellfish free and the recipe has oyster sauce in it\n",
    "    - Is there a harm to the user?  What if they don't know oyster sauce often has shellfish?\n",
    "    - Does this effect churn/subscription liklihood?  By a little or a lot..\n",
    "    \n",
    "#### The model create a complient recipe and thinks it is not-compliant \n",
    "\n",
    "- A user asked for for a dairy-free recipe, and the generated recipe was dairy-free but my judge thought it wasn't.\n",
    "    - What is the product behavior?  \n",
    "        - Would it show the recipe anyway and it be fine?\n",
    "        - Do you have logic to ask it to try again, meaning increased latency percieved to the user?\n",
    "        - Would it explain the failure to the user?\n",
    "    - Based on the product behavior:\n",
    "        - What's the churn/subscription liklihood risk?\n",
    "        - Is there any real harm to the user?\n",
    "        - Can they still use the product with an obvious workaround, or are they completely stuck?\n",
    "\n",
    "#### Decision\n",
    "\n",
    "Once you have really thought about those, consider what kind of error is more painful?  Which do you care more about?  Do you care about each equally, or is one drastically more important to your product than the other?  \n",
    "\n",
    "Communicate this reasoning clearly to everyone involved!\n",
    "\n",
    "\n",
    "## Analyze Test Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a judge we are happy with, we can measure on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze test predictions to understand errors\n",
    "with open(\"results/test_predictions.json\", \"r\") as f:\n",
    "    test_preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives = [\n",
    "    p for p in test_preds if p[\"true_label\"] == \"PASS\" and p[\"predicted_label\"] == \"FAIL\"\n",
    "]\n",
    "false_positives = [\n",
    "    p for p in test_preds if p[\"true_label\"] == \"FAIL\" and p[\"predicted_label\"] == \"PASS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"False Negatives: {len(false_negatives)}\")\n",
    "print(f\"False Positives: {len(false_positives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how many failures we had!\n",
    "\n",
    "> ❗️ Remember, the more we look at the test set the less it protects us against overfitting.  So what we're going to do next is NOT what you should do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a failure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(false_positives[1][\"dietary_restriction\"])\n",
    "print(false_positives[1][\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we look we can clearly see the failure mode.  This is dairy free, but there's an optional cheese topping!\n",
    "\n",
    "But what now?\n",
    "\n",
    "In production, I don't get to see a user query and change the system to better handle that specific user query.  So I shouldn't be able to do that on the test set!  The problem is I am biased.  I know that optional cheese topping is a specific failure mode.  I suspect it might be all optional topics that it might miss.  Even if I don't add this example, this failure will be top of mind and my way of improving performance is biased to this failure in my test set.  It's a bit like hardcoding a solution.\n",
    "\n",
    "So what do I do when I really need to look at the test set?\n",
    "\n",
    "- Accept my future tests are compromised\n",
    "- Turn the test set into a larger training set and make a new test set.  If this was a surprising failure mode, then we haven't reached saturation so we should go look at more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure on \"New\" Traces\n",
    "\n",
    "Now we can run on more traces that don't have ground truth to estimate how well out production traces adhere to to our criteria using our judge.  This is done in `run_full_evaluation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final evaluation results\n",
    "with open(\"results/final_evaluation.json\", \"r\") as f:\n",
    "    final_eval = json.load(f)\n",
    "\n",
    "eval_results = final_eval[\"final_evaluation\"]\n",
    "\n",
    "print(f\"Total traces evaluated: {eval_results['total_traces_evaluated']}\")\n",
    "print(\n",
    "    f\"Raw observed success rate: {eval_results['raw_observed_success_rate']:.3f} ({eval_results['raw_observed_success_rate'] * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's a problem!  We know our judge isn't perfect, so we should account for that in what it tells us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Results with judgy\n",
    "\n",
    "The `judgy` library was used to correct for judge bias based on the TPR/TNR from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our judge was 90% accurate based on evaluations on our labeled data, and when run on production data it said we were 100% correct, then **we should be skeptical of that 100% result**.\n",
    "\n",
    "Here's the intuitive reasoning:\n",
    "\n",
    "**The Problem:** Our judge makes mistakes 10% of the time. So when it says \"everything is perfect\" on production data, some of those \"perfect\" judgments are probably wrong.\n",
    "\n",
    "**What's Really Happening:** \n",
    "- The judge is likely missing some actual violations (false negatives)\n",
    "- It might also be flagging some good responses as bad (false positives)\n",
    "- The 100% \"success rate\" is inflated because the judge isn't catching all the real problems\n",
    "\n",
    "**The Correction:**\n",
    "Since we know our judge's error patterns from the labeled test data, we can mathematically adjust the 100% observed rate to estimate what the *true* success rate probably is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we know our judge isn't perfect, so we should account for that when we use it for measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import judgy to understand the correction\n",
    "\n",
    "# Display the corrected results\n",
    "print(\"\\nCorrected Results (accounting for judge bias):\")\n",
    "print(\n",
    "    f\"Corrected success rate: {eval_results['corrected_success_rate']:.3f} ({eval_results['corrected_success_rate'] * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"95% Confidence Interval: [{eval_results['confidence_interval_95']['lower_bound']:.3f}, {eval_results['confidence_interval_95']['upper_bound']:.3f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"                        [{eval_results['confidence_interval_95']['lower_bound'] * 100:.1f}%, {eval_results['confidence_interval_95']['upper_bound'] * 100:.1f}%]\"\n",
    ")\n",
    "\n",
    "correction = eval_results[\"corrected_success_rate\"] - eval_results[\"raw_observed_success_rate\"]\n",
    "print(f\"\\nCorrection applied: {correction:.3f} ({correction * 100:.1f} percentage points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals Made Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is sampling ok instead of statistical formulas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All statistical measures you need for evaluations can be derived through sampling.  This is beneficial because:\n",
    "\n",
    "1. You avoid assumptions about your data that are baked into statistical formulas, which can be hard to get right\n",
    "2. It is easy to reason about and change your evaluations\n",
    "3. It's faster and easier to explain how it works to non-statisticians\n",
    "\n",
    "Often people think this approach I am going to show is not the \"right\" way, but here's what the R.A. Fisher (\"father\" of modern statistics) had this to say about it why the complex statistical formulas were used!\n",
    "\n",
    "> Actually, the statistician does not carry out this very simple and very tedious process, but his conclusions have no justification beyond the fact that they agree with those which could have\n",
    "been arrived at by this elementary method.\n",
    "\n",
    "This is discussing sampling being too tedious in 1936.  Today, it's as simple as `for i in range(10000)`.  It's not tedious anymore, so we don't need to complicated computational shortcuts.  We can do things the simple way :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple dataset.  We will have 17 passes (1), and 3 failures (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data: 1 = pass, 0 = fail\n",
    "results = [1] * 17 + [0] * 3\n",
    "observed_rate = np.mean(results)\n",
    "print(f\"Observed success rate: {observed_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean is 85%, meaning we have an 85% success rate.  The question is how much ambiguity is there? If I re-did the experiment that got 85% sucess rate over and over, what range of successes would I get?  If it pretty much always going to be 85% success?  Or would it wildly swing and have lots of noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check by sampling.  We pick a random item from our list of 20 results. And we do that 20 times.  Sometimes we'll get unlucky and pick lots of `0`'s.  Sometimes we'll get lucky and get very few.  Let's try it 10,000 times to see how much luck has to do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap in 5 lines\n",
    "bootstrap_rates = []\n",
    "for _ in range(10000):\n",
    "    resample = np.random.choice(results, size=len(results), replace=True)\n",
    "    bootstrap_rates.append(np.mean(resample))\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower = np.percentile(bootstrap_rates, 2.5)\n",
    "ci_upper = np.percentile(bootstrap_rates, 97.5)\n",
    "\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.1%}, {ci_upper:.1%}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!  95% of the time my values are between 70% and 100%.  That's a huge range.  I don't feel good about that, because 70% accuracy seems unnacceptable, but 100% accuracy seems suspicious.\n",
    "\n",
    "**What can I do?**\n",
    "\n",
    "This is *yet another* example where just getting more data and labeling it solves a lot of problems!  The more data you have, the less uncertainty in your results!  Let's keep the same 85% success rate, but with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data: 1 = pass, 0 = fail\n",
    "results = [1] * 170 + [0] * 30  # 850 passes, 150 fails\n",
    "observed_rate = np.mean(results)\n",
    "print(f\"Observed success rate: {observed_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the example same sampling, but with the larger dataset (200 labeled examples instead of 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap in 5 lines\n",
    "bootstrap_rates = []\n",
    "for _ in range(10000):\n",
    "    resample = np.random.choice(results, size=len(results), replace=True)\n",
    "    bootstrap_rates.append(np.mean(resample))\n",
    "\n",
    "# Get 95% confidence interval\n",
    "ci_lower = np.percentile(bootstrap_rates, 2.5)\n",
    "ci_upper = np.percentile(bootstrap_rates, 97.5)\n",
    "\n",
    "print(f\"95% Confidence Interval: [{ci_lower:.1%}, {ci_upper:.1%}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better!\n",
    "\n",
    "To improve the accuracy of your confidence interval, you can increase the number of loops you do.  But generally this is plenty accurate.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
