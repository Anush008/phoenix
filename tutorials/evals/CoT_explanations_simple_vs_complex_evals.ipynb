{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a846d8",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\"> Judge Prompt Comparison: Simple/Complex × Reasoning/Non-Reasoning Models </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e7025",
   "metadata": {},
   "source": [
    "This notebook uses **Arize Phoenix `llm_classify`** to evaluate tool-calling predictions on the **Berkeley Function Calling Leaderboard (BFCL)** dataset with:\n",
    "- a **simple** binary prompt (`Yes`/`No`), and\n",
    "- a **complex** multi-class prompt (`correct` / `partially_correct` / `incorrect`).\n",
    "\n",
    "We keep it minimal and focused on classification-style LLM-as-a-judge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052dc541",
   "metadata": {},
   "source": [
    "## 1) Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b00e731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q install --upgrade pandas datasets arize-phoenix-evals openai tiktoken nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7185b",
   "metadata": {},
   "source": [
    "## 2) Configure Judge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f9c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"⚠️ Set OPENAI_API_KEY env var to run evals.\")\n",
    "\n",
    "non_reasoning_model = OpenAIModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0,\n",
    ")\n",
    "reasoning_model = OpenAIModel(\n",
    "    model=\"o3\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b57f78",
   "metadata": {},
   "source": [
    "## 3) Load BFCL (V3 Exec Splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994eb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple shape: (100, 5) Multiple shape: (50, 5)\n",
      "Combined shape: (150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>function</th>\n",
       "      <th>execution_result_type</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exec_simple_0</td>\n",
       "      <td>[[{'role': 'user', 'content': 'I've been playi...</td>\n",
       "      <td>[{'name': 'calc_binomial_probability', 'descri...</td>\n",
       "      <td>[exact_match]</td>\n",
       "      <td>[calc_binomial_probability(n=20, k=5, p=0.6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exec_simple_1</td>\n",
       "      <td>[[{'role': 'user', 'content': 'During last nig...</td>\n",
       "      <td>[{'name': 'calc_binomial_probability', 'descri...</td>\n",
       "      <td>[exact_match]</td>\n",
       "      <td>[calc_binomial_probability(n=30, k=15, p=0.5)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           question  \\\n",
       "0  exec_simple_0  [[{'role': 'user', 'content': 'I've been playi...   \n",
       "1  exec_simple_1  [[{'role': 'user', 'content': 'During last nig...   \n",
       "\n",
       "                                            function execution_result_type  \\\n",
       "0  [{'name': 'calc_binomial_probability', 'descri...         [exact_match]   \n",
       "1  [{'name': 'calc_binomial_probability', 'descri...         [exact_match]   \n",
       "\n",
       "                                     ground_truth  \n",
       "0   [calc_binomial_probability(n=20, k=5, p=0.6)]  \n",
       "1  [calc_binomial_probability(n=30, k=15, p=0.5)]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BFCL_FILES = {\n",
    "    \"exec_simple\": \"https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard/resolve/main/BFCL_v3_exec_simple.json\",\n",
    "    \"exec_multiple\": \"https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard/resolve/main/BFCL_v3_exec_multiple.json\",\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_json(url, out_path):\n",
    "    out = Path(out_path)\n",
    "    if not out.exists():\n",
    "        print(f\"Downloading {url} -> {out}\")\n",
    "        urllib.request.urlretrieve(url, out)\n",
    "    text = Path(out).read_text()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        rows = []\n",
    "        for line in text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                pass\n",
    "        return rows\n",
    "\n",
    "\n",
    "df_simple = pd.DataFrame(fetch_json(BFCL_FILES[\"exec_simple\"], \"BFCL_v3_exec_simple.json\"))\n",
    "df_multi = pd.DataFrame(fetch_json(BFCL_FILES[\"exec_multiple\"], \"BFCL_v3_exec_multiple.json\"))\n",
    "print(\"Simple shape:\", df_simple.shape, \"Multiple shape:\", df_multi.shape)\n",
    "df = pd.concat([df_simple, df_multi], ignore_index=True)\n",
    "print(\"Combined shape:\", df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a35ff9",
   "metadata": {},
   "source": [
    "## 4) Prepare DataFrame (instruction, functions, ground truth, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e73a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                        instruction  \\\n",
      "0  exec_multiple_49  I have a set of vertices: [[1,2],[3,4],[1,4],[...   \n",
      "1    exec_simple_84  I need to identify the straight line that cont...   \n",
      "2    exec_simple_40  I'm currently working on a detailed city map, ...   \n",
      "\n",
      "                                      functions_json  \\\n",
      "0  [{\"name\": \"convert_coordinates\", \"parameters\":...   \n",
      "1  [{\"name\": \"maxPoints\", \"parameters\": {\"type\": ...   \n",
      "2  [{\"name\": \"get_distance\", \"parameters\": {\"type...   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0   polygon_area(vertices=[[1,2],[3,4],[1,4],[3,7]])   \n",
      "1        maxPoints(points=[[1,1],[2,2],[3,4],[5,5]])   \n",
      "2  get_distance(pointA=(45.76, 4.85), pointB=(48....   \n",
      "\n",
      "                                      pred_tool_call  \n",
      "0   polygon_area(vertices=[[1,2],[3,4],[1,4],[3,7]])  \n",
      "1     maxPoints (points=[[1.1,1],[2,2],[3,4],[5,5]])  \n",
      "2  get_distance(pointA=(45.76, 4.85), pointB=(48....  \n"
     ]
    }
   ],
   "source": [
    "def extract_gt_call(row):\n",
    "    gt = row.get(\"ground_truth\")\n",
    "    if isinstance(gt, list) and gt:\n",
    "        return gt[0]\n",
    "    if isinstance(gt, str):\n",
    "        return gt\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def extract_functions(row):\n",
    "    fns = row.get(\"function\", [])\n",
    "    if isinstance(fns, dict):\n",
    "        fns = [fns]\n",
    "    return fns\n",
    "\n",
    "\n",
    "def extract_instruction(row):\n",
    "    q = row.get(\"question\", [])\n",
    "    last_user = \"\"\n",
    "    for msg_list in q:\n",
    "        for m in msg_list:\n",
    "            if m.get(\"role\") == \"user\":\n",
    "                last_user = m.get(\"content\", last_user)\n",
    "    return last_user\n",
    "\n",
    "\n",
    "work = []\n",
    "for _, r in df.iterrows():\n",
    "    rr = r.to_dict()\n",
    "    work.append(\n",
    "        {\n",
    "            \"id\": rr.get(\"id\", \"\"),\n",
    "            \"instruction\": extract_instruction(rr),\n",
    "            \"functions_json\": json.dumps(\n",
    "                [\n",
    "                    {\n",
    "                        \"name\": f.get(\"name\"),\n",
    "                        \"parameters\": f.get(\"parameters\"),\n",
    "                        \"description\": f.get(\"description\", \"\"),\n",
    "                    }\n",
    "                    for f in extract_functions(rr)\n",
    "                ],\n",
    "                ensure_ascii=False,\n",
    "            ),\n",
    "            \"ground_truth\": extract_gt_call(rr),\n",
    "        }\n",
    "    )\n",
    "wf = pd.DataFrame(work)\n",
    "\n",
    "PREDICTIONS_CSV = os.getenv(\"PREDICTIONS_CSV\", \"\")\n",
    "if PREDICTIONS_CSV and Path(PREDICTIONS_CSV).exists():\n",
    "    preds = pd.read_csv(PREDICTIONS_CSV)[[\"id\", \"pred_tool_call\"]]\n",
    "    data = wf.merge(preds, on=\"id\", how=\"left\")\n",
    "else:\n",
    "\n",
    "    def corrupt_call(s: str) -> str:\n",
    "        if not s or \"(\" not in s:\n",
    "            return s\n",
    "        tool, args = s.split(\"(\", 1)\n",
    "        tool = tool.strip()\n",
    "        args = args.rstrip(\")\")\n",
    "        if random.random() < 0.5:\n",
    "            tool = tool + \"_alt\"\n",
    "        else:\n",
    "            args = re.sub(r\"(\\d+(?:\\.\\d+)?)\", lambda m: str(float(m.group()) * 1.1), args, count=1)\n",
    "        return f\"{tool} ({args})\"\n",
    "\n",
    "    pred_tool_call = [\n",
    "        gt if random.random() < 0.7 else corrupt_call(gt) for gt in wf[\"ground_truth\"]\n",
    "    ]\n",
    "    data = wf.copy()\n",
    "    data[\"pred_tool_call\"] = pred_tool_call\n",
    "\n",
    "SAMPLE = int(os.getenv(\"EVAL_SAMPLE\", \"100\"))\n",
    "data = data.sample(min(SAMPLE, len(data)), random_state=7).reset_index(drop=True)\n",
    "\n",
    "print(data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d366c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset shape: (30, 5)\n",
      "First 3 rows of sampled data:\n"
     ]
    }
   ],
   "source": [
    "small_data = data.sample(n=30, random_state=42).reset_index(drop=True)\n",
    "print(f\"Sampled dataset shape: {small_data.shape}\")\n",
    "print(\"First 3 rows of sampled data:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc0086",
   "metadata": {},
   "source": [
    "## 5) Define your LLM-as-a-Judge Templates & Rails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ae0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_TEMPLATE = \"\"\"You are grading a tool-calling attempt.\n",
    "\n",
    "Given:\n",
    "USER INSTRUCTION:\n",
    "{instruction}\n",
    "\n",
    "AVAILABLE FUNCTIONS (JSON Schemas):\n",
    "{functions_json}\n",
    "\n",
    "MODEL TOOL CALL (string):\n",
    "{pred_tool_call}\n",
    "\n",
    "GROUND TRUTH TOOL CALL (string):\n",
    "{ground_truth}\n",
    "\n",
    "Question: Did the model invoke the correct tool(s) AND use the correct parameter names and values?\n",
    "Answer strictly with one token: Yes or No.\n",
    "\"\"\"\n",
    "\n",
    "SIMPLE_RAILS = [\"Yes\", \"No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLEX_TEMPLATE = \"\"\"You are grading a tool-calling attempt.\n",
    "Return ONLY one of the following labels:\n",
    "- correct\n",
    "- partially_correct\n",
    "- incorrect\n",
    "\n",
    "Use these rules:\n",
    "- Consider types and trivial formatting (e.g., '5' vs 5, whitespace) as equivalent.\n",
    "- Consider equivalent units only if explicitly clear from context.\n",
    "- The attempt is \"correct\" only if the tool and all required parameters match the ground truth.\n",
    "- It's \"partially_correct\" if the tool is correct but parameters have minor issues.\n",
    "- It's \"incorrect\" otherwise.\n",
    "\n",
    "Context:\n",
    "USER INSTRUCTION:\n",
    "{instruction}\n",
    "\n",
    "AVAILABLE FUNCTIONS (JSON Schemas):\n",
    "{functions_json}\n",
    "\n",
    "MODEL TOOL CALL (string):\n",
    "{pred_tool_call}\n",
    "\n",
    "GROUND TRUTH TOOL CALL (string):\n",
    "{ground_truth}\n",
    "\"\"\"\n",
    "\n",
    "COMPLEX_RAILS = [\"correct\", \"partially_correct\", \"incorrect\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79bb08",
   "metadata": {},
   "source": [
    "## 6) Run `llm_classify` for the Simple Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722232e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d703d5af294d67b62771b254d36686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/30 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>response</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>The model did not invoke the correct tool or u...</td>\n",
       "      <td>{\"explanation\":\"The model did not invoke the c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>1.086622</td>\n",
       "      <td>154</td>\n",
       "      <td>30</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The model did not invoke the correct tool or u...</td>\n",
       "      <td>{\"explanation\":\"The model did not invoke the c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>0.969510</td>\n",
       "      <td>154</td>\n",
       "      <td>30</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>The model did not invoke the correct tool or u...</td>\n",
       "      <td>{\"explanation\":\"The model did not invoke the c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>1.560811</td>\n",
       "      <td>154</td>\n",
       "      <td>30</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        explanation  \\\n",
       "0    no  The model did not invoke the correct tool or u...   \n",
       "1    no  The model did not invoke the correct tool or u...   \n",
       "2    no  The model did not invoke the correct tool or u...   \n",
       "\n",
       "                                            response exceptions  \\\n",
       "0  {\"explanation\":\"The model did not invoke the c...         []   \n",
       "1  {\"explanation\":\"The model did not invoke the c...         []   \n",
       "2  {\"explanation\":\"The model did not invoke the c...         []   \n",
       "\n",
       "  execution_status  execution_seconds  prompt_tokens  completion_tokens  \\\n",
       "0        COMPLETED           1.086622            154                 30   \n",
       "1        COMPLETED           0.969510            154                 30   \n",
       "2        COMPLETED           1.560811            154                 30   \n",
       "\n",
       "   total_tokens  \n",
       "0           184  \n",
       "1           184  \n",
       "2           184  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_df = small_data.copy()\n",
    "\n",
    "non_reasoning_simple_results = llm_classify(\n",
    "    data=simple_df.assign(template=SIMPLE_TEMPLATE),\n",
    "    model=non_reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=SIMPLE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "\n",
    "non_reasoning_simple_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae98d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ef114238bb417fa43553372e2fe11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/30 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>response</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>The prompt did not provide the actual user ins...</td>\n",
       "      <td>{\"explanation\":\"The prompt did not provide the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>7.555468</td>\n",
       "      <td>148</td>\n",
       "      <td>403</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>Unable to directly inspect the predicted and g...</td>\n",
       "      <td>{\"explanation\":\"Unable to directly inspect the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>11.894640</td>\n",
       "      <td>148</td>\n",
       "      <td>576</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>The necessary information (instruction, functi...</td>\n",
       "      <td>{\"response\":\"No\",\"explanation\":\"The necessary ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>9.004514</td>\n",
       "      <td>148</td>\n",
       "      <td>580</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        explanation  \\\n",
       "0    no  The prompt did not provide the actual user ins...   \n",
       "1    no  Unable to directly inspect the predicted and g...   \n",
       "2    no  The necessary information (instruction, functi...   \n",
       "\n",
       "                                            response exceptions  \\\n",
       "0  {\"explanation\":\"The prompt did not provide the...         []   \n",
       "1  {\"explanation\":\"Unable to directly inspect the...         []   \n",
       "2  {\"response\":\"No\",\"explanation\":\"The necessary ...         []   \n",
       "\n",
       "  execution_status  execution_seconds  prompt_tokens  completion_tokens  \\\n",
       "0        COMPLETED           7.555468            148                403   \n",
       "1        COMPLETED          11.894640            148                576   \n",
       "2        COMPLETED           9.004514            148                580   \n",
       "\n",
       "   total_tokens  \n",
       "0           551  \n",
       "1           724  \n",
       "2           728  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_df = small_data.copy()\n",
    "\n",
    "reasoning_simple_results = llm_classify(\n",
    "    data=simple_df.assign(template=SIMPLE_TEMPLATE),\n",
    "    model=reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=SIMPLE_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "\n",
    "reasoning_simple_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b1988",
   "metadata": {},
   "source": [
    "## 6.5) Run `llm_classify` for the Complex Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1b05f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c9eeedb635440cb5ada01092d7b84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/30 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>response</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>The tool and parameters in the model tool call...</td>\n",
       "      <td>{\"explanation\":\"The tool and parameters in the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>1.530556</td>\n",
       "      <td>230</td>\n",
       "      <td>41</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>The tool and parameters in the model tool call...</td>\n",
       "      <td>{\"explanation\":\"The tool and parameters in the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>1.437953</td>\n",
       "      <td>230</td>\n",
       "      <td>47</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>The tool and parameters in the model tool call...</td>\n",
       "      <td>{\"explanation\":\"The tool and parameters in the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>1.714084</td>\n",
       "      <td>230</td>\n",
       "      <td>47</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                        explanation  \\\n",
       "0  incorrect  The tool and parameters in the model tool call...   \n",
       "1  incorrect  The tool and parameters in the model tool call...   \n",
       "2  incorrect  The tool and parameters in the model tool call...   \n",
       "\n",
       "                                            response exceptions  \\\n",
       "0  {\"explanation\":\"The tool and parameters in the...         []   \n",
       "1  {\"explanation\":\"The tool and parameters in the...         []   \n",
       "2  {\"explanation\":\"The tool and parameters in the...         []   \n",
       "\n",
       "  execution_status  execution_seconds  prompt_tokens  completion_tokens  \\\n",
       "0        COMPLETED           1.530556            230                 41   \n",
       "1        COMPLETED           1.437953            230                 47   \n",
       "2        COMPLETED           1.714084            230                 47   \n",
       "\n",
       "   total_tokens  \n",
       "0           271  \n",
       "1           277  \n",
       "2           277  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_df = small_data.copy()\n",
    "\n",
    "non_reasoning_complex_results = llm_classify(\n",
    "    data=complex_df.assign(template=COMPLEX_TEMPLATE),\n",
    "    model=non_reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=COMPLEX_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "non_reasoning_complex_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebbe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7883965fdc42aa8be5af0b16f47007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/30 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>response</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>No information about the predicted and ground-...</td>\n",
       "      <td>{\"response\":\"incorrect\",\"explanation\":\"No info...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>4.159545</td>\n",
       "      <td>224</td>\n",
       "      <td>181</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>Cannot compare because the prediction or groun...</td>\n",
       "      <td>{\"response\":\"incorrect\",\"explanation\":\"Cannot ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>5.005243</td>\n",
       "      <td>224</td>\n",
       "      <td>235</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>Insufficient data to compare predicted tool ca...</td>\n",
       "      <td>{\"response\":\"incorrect\",\"explanation\":\"Insuffi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>3.929234</td>\n",
       "      <td>224</td>\n",
       "      <td>171</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                        explanation  \\\n",
       "0  incorrect  No information about the predicted and ground-...   \n",
       "1  incorrect  Cannot compare because the prediction or groun...   \n",
       "2  incorrect  Insufficient data to compare predicted tool ca...   \n",
       "\n",
       "                                            response exceptions  \\\n",
       "0  {\"response\":\"incorrect\",\"explanation\":\"No info...         []   \n",
       "1  {\"response\":\"incorrect\",\"explanation\":\"Cannot ...         []   \n",
       "2  {\"response\":\"incorrect\",\"explanation\":\"Insuffi...         []   \n",
       "\n",
       "  execution_status  execution_seconds  prompt_tokens  completion_tokens  \\\n",
       "0        COMPLETED           4.159545            224                181   \n",
       "1        COMPLETED           5.005243            224                235   \n",
       "2        COMPLETED           3.929234            224                171   \n",
       "\n",
       "   total_tokens  \n",
       "0           405  \n",
       "1           459  \n",
       "2           395  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_df = small_data.copy()\n",
    "\n",
    "reasoning_complex_results = llm_classify(\n",
    "    data=complex_df.assign(template=COMPLEX_TEMPLATE),\n",
    "    model=reasoning_model,\n",
    "    template=\"{template}\",\n",
    "    rails=COMPLEX_RAILS,\n",
    "    provide_explanation=True,\n",
    "    include_prompt=False,\n",
    "    include_response=True,\n",
    "    run_sync=True,\n",
    ")\n",
    "\n",
    "reasoning_complex_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b69a6c",
   "metadata": {},
   "source": [
    "## 7) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dcd3002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Simple Eval: \n",
      "Reasoning and non-reasoning models agree on all samples\n",
      "Non-reasoning model used 5520 tokens\n",
      "Reasoning model used 18704 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"For Simple Eval: \")\n",
    "different_labels = (\n",
    "    non_reasoning_simple_results[\"label\"] != reasoning_simple_results[\"label\"]\n",
    ").sum()\n",
    "if different_labels == 0:\n",
    "    print(\"Reasoning and non-reasoning models agree on all samples\")\n",
    "else:\n",
    "    print(f\"Reasoning and non-reasoning models disagree on {different_labels} samples\")\n",
    "NR_simple_lokens = non_reasoning_simple_results[\"total_tokens\"].sum()\n",
    "R_simple_lokens = reasoning_simple_results[\"total_tokens\"].sum()\n",
    "print(f\"Non-reasoning model used {NR_simple_lokens} tokens\")\n",
    "print(f\"Reasoning model used {R_simple_lokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4af64f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Complex Eval: \n",
      "Reasoning and non-reasoning models disagree on 7 samples\n",
      "Non-reasoning model used 8184 tokens\n",
      "Reasoning model used 21627 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"For Complex Eval: \")\n",
    "different_labels = (\n",
    "    non_reasoning_complex_results[\"label\"] != reasoning_complex_results[\"label\"]\n",
    ").sum()\n",
    "if different_labels == 0:\n",
    "    print(\"Reasoning and non-reasoning models agree on all samples\")\n",
    "else:\n",
    "    print(f\"Reasoning and non-reasoning models disagree on {different_labels} samples\")\n",
    "NR_complex_tokens = non_reasoning_complex_results[\"total_tokens\"].sum()\n",
    "R_complex_tokens = reasoning_complex_results[\"total_tokens\"].sum()\n",
    "print(f\"Non-reasoning model used {NR_complex_tokens} tokens\")\n",
    "print(f\"Reasoning model used {R_complex_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c84e99",
   "metadata": {},
   "source": [
    "### References\n",
    "- `llm_classify` API (Phoenix Evals): https://arize-phoenix.readthedocs.io/en/latest/api/evals.classify.html\n",
    "- Phoenix Evals Overview: https://arize.com/docs/phoenix/evaluation/llm-evals\n",
    "- Using `llm_classify` (Docs): https://arize.com/docs/phoenix/evaluation/how-to-evals/bring-your-own-evaluator\n",
    "- BFCL dataset: https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
