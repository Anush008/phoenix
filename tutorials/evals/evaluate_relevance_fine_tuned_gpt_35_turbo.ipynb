{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Fine-Tuning GPT-3.5-Turbo to Evaluate Retrieval-Augmented Generation (RAG) Applications</h1>\n",
    "\n",
    "This notebook shows how to fine-tune a GPT-3.5-Turbo base model to match the performance of GPT-4 on the task of relevance classification.\n",
    "\n",
    "‚ÑπÔ∏è This notebook requires an OpenAI key.\n",
    "\n",
    "‚ö†Ô∏è Fine-tuning may an hour or longer and will cost a few dollars.\n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "Arize provides tooling to evaluate LLM applications, including tools to determine the relevance or irrelevance of documents retrieved by retrieval-augmented generation (RAG) applications. This relevance is then used to measure the quality of each retrieval using ranking metrics such as precision@k. In order to determine whether each retrieved document is relevant or irrelevant to the corresponding query, our approach is straightforward: ask an LLM.\n",
    "\n",
    "To maximize throughput and minimize cost, it's desirable to keep the prompt short and preferably zero-shot, meaning that no concrete examples are included in the prompt. As you'll see, GPT-4 performs well at the task of relevance classification even with a zero-shot prompt, while GPT-3.5-Turbo struggles. On the other hand, GPT-4 is slower, has lower rate limits, and is more expensive that GPT-3.5-Turbo.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Fine-tune GPT-3.5-Turbo on WikiQA, a question-answering dataset that contains queries, retrieved documents, and ground truth binary relevance labels from human labelers.\n",
    "- Evaluate your fine-tuned model against GPT-3.5-turbo and GPT-4 base models on a holdout test set.\n",
    "\n",
    "Once you've fine-tuned GPT-3.5-Turbo, you can then use it to evaluate your RAG applications with higher volume and lower cost than if you were using GPT-4.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[experimental]==0.0.33rc9\" ipython matplotlib openai scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "from phoenix.experimental.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n",
    "    OpenAiModel,\n",
    "    PromptTemplate,\n",
    "    download_benchmark_dataset,\n",
    "    llm_eval_binary,\n",
    ")\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Your OpenAI API Key\n",
    "\n",
    "Set your OpenAI API key if it is not already set as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Benchmark Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the WikiQA training dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_df = (\n",
    "    download_benchmark_dataset(task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"query_text\": \"query\",\n",
    "            \"document_text\": \"reference\",\n",
    "        },\n",
    "    )\n",
    "    .sample(frac=1.0, random_state=42)\n",
    ")\n",
    "fine_tune_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and sample the WikiQA test dataset for evaluation of the fine-tuned model against base GPT-3.5-turbo and GPT-4 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = (\n",
    "    download_benchmark_dataset(task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-test\")\n",
    "    .sample(n=10, random_state=42)  # FIXME\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"query_text\": \"query\",\n",
    "            \"document_text\": \"reference\",\n",
    "        },\n",
    "    )\n",
    ")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Your Fine-Tuning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format your prompt template across the WikiQA training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(RAG_RELEVANCY_PROMPT_TEMPLATE_STR)\n",
    "prompts = fine_tune_df.apply(lambda record: prompt_template.format(record), axis=1).tolist()\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI API expects fine-tuning data to come as a sequence of conversations, each conversation being a list of chat message objects, in JSONL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = fine_tune_df[\"relevant\"].map({True: \"relevant\", False: \"irrelevant\"})\n",
    "fine_tune_examples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": actual,\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    for prompt, actual in zip(prompts, actuals)\n",
    "]\n",
    "print(json.dumps(fine_tune_examples[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the fine-tuning data to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with StringIO() as buffer:\n",
    "    for example in fine_tune_examples[:50]:  # FIXME\n",
    "        buffer.write(json.dumps(example) + \"\\n\")\n",
    "    buffer.seek(0)\n",
    "\n",
    "    file_response = openai.File.create(\n",
    "        file=buffer,\n",
    "        purpose=\"fine-tune\",\n",
    "    )\n",
    "    file_id = file_response.to_dict()[\"id\"]\n",
    "\n",
    "print(f\"File ID: {file_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a fine-tuning job.\n",
    "\n",
    "‚ö†Ô∏è You might have to wait for the file upload to finish processing before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response = openai.FineTuningJob.create(\n",
    "    training_file=file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=\"wikiqa-relevance\",\n",
    ")\n",
    "job_id = job_response.to_dict()[\"id\"]\n",
    "print(f\"Job ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning job may take up to an hour or more. You will receive an email when it's done. Once your job is done, the following cell should run and you can fetch the name of your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_name = job.fine_tuned_model\n",
    "\n",
    "print(f\"Your job's status is: {job.status}\")\n",
    "assert job.status == \"succeeded\", \"Your fine-tuning job has failed or is in progress.\"\n",
    "assert fine_tuned_model_name is not None\n",
    "print(f'Fine-tuned model name: \"{fine_tuned_model_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Your LLMs\n",
    "\n",
    "Display the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RAG_RELEVANCY_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template variables are:\n",
    "\n",
    "- **query_text:** the question asked by a user\n",
    "- **document_text:** the text of the retrieved document\n",
    "- **relevant:** a ground-truth binary relevance label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate LLMs for the fine-tuned model and base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"gpt-3.5-turbo\", \"gpt-4\", fine_tuned_model_name]\n",
    "llms = {\n",
    "    model_name: OpenAiModel(\n",
    "        model_name=model_name,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    for model_name in model_names\n",
    "}\n",
    "llms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Run relevance classifications against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_classifications = {}\n",
    "rails = [\"relevant\", \"irrelevant\"]\n",
    "for model_name, model in llms.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    relevance_classifications[model_name] = llm_eval_binary(\n",
    "        dataframe=test_df,\n",
    "        template=RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n",
    "        model=model,\n",
    "        rails=rails,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifications\n",
    "\n",
    "Evaluate the predictions against human-labeled ground-truth relevance labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Confusion Matrices\")\n",
    "\n",
    "for model_index, model_name in enumerate(model_names):\n",
    "    predictions = relevance_classifications[model_name]\n",
    "\n",
    "    print(model_name)\n",
    "    print(\"=\" * len(model_name))\n",
    "    print(classification_report(actuals, predictions, labels=rails))\n",
    "    print()\n",
    "\n",
    "    ax = axes[model_index]\n",
    "    ax.set_title(model_name)\n",
    "    conf_mat = confusion_matrix(actuals, predictions, labels=rails)\n",
    "    conf_mat_disp = ConfusionMatrixDisplay(conf_mat, display_labels=rails)\n",
    "    conf_mat_disp.plot(ax=ax, cmap=\"Blues\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
