{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating Agent Sessions with Phoenix</h1>\n",
    "\n",
    "This notebook provides an end-to-end example of how to evaluate agent sessions using Phoenix. The workflow demonstrates:\n",
    "\n",
    "* Connecting to a Phoenix instance and downloading session-based trace data\n",
    "* Filtering and preparing session data for evaluation\n",
    "* Defining and running an LLM-based evaluation (using OpenAI) to classify session correctness based on coherence, context maintenance, and goal achievement\n",
    "* Merging evaluation results and optionally logging them back to Phoenix for visualization and analysis\n",
    "* Saving the combined results locally for further analysis\n",
    "\n",
    "This example focuses on evaluating the overall effectiveness and correctness of agent sessions, rather than just individual traces or function calls.\n",
    "\n",
    "## Install Dependencies, Import Libraries, configure API keys, and launch Phoenix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq arize-phoenix \"arize-phoenix[evals]>=8.8.0\" \"openai>=1\" nest_asyncio pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from getpass import getpass\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "# Enable phoenix.evals concurrency inside Jupyter Notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Configure Phoenix (optional for hosted Phoenix)\n",
    "if not os.getenv(\"PHOENIX_API_KEY\"):\n",
    "    phoenix_api_key = getpass(\n",
    "        \"🔑 Enter your Phoenix API key (or press Enter to skip for local Phoenix): \"\n",
    "    )\n",
    "    if phoenix_api_key:\n",
    "        os.environ[\"PHOENIX_API_KEY\"] = phoenix_api_key\n",
    "\n",
    "# Configuration parameters\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "PROJECT_NAME = \"YOUR_PROJECT_NAME\"\n",
    "\n",
    "# Set Phoenix endpoint (adjust for your setup)\n",
    "# For local Phoenix, use: \"http://localhost:6006\"\n",
    "# For hosted Phoenix use\"https://app.phoenix.arize.com\"\n",
    "if not os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\"):\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Session Data from Phoenix\n",
    "\n",
    "client = px.Client()\n",
    "\n",
    "# Pull spans for the last X days to avoid gigantic dataframes. Adjust as needed.\n",
    "START_TIME = datetime.now() - timedelta(days=7)\n",
    "END_TIME = datetime.now()\n",
    "\n",
    "print(f\"📊 Loading session data from project: {PROJECT_NAME}\")\n",
    "primary_df = client.query_spans(start_time=START_TIME, end_time=END_TIME, project_name=PROJECT_NAME)\n",
    "\n",
    "print(f\"   → Retrieved {len(primary_df):,} spans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "The following helper functions are used to process session data for evaluation:\n",
    "\n",
    "- **`_apply_filter(df, column, operator, value)`**: Lightweight filter helper supporting common pandas DataFrame operations (`==`, `!=`, `contains`, `isna`, `notna`). Used to filter spans by specific criteria like span kind, status, or attributes.\n",
    "\n",
    "- **`filter_sessions_by_trace_criteria(df, trace_filters, span_filters)`**: Used to identify sessions that meet evaluation criteria (e.g., sessions with root spans only). Returns all spans for sessions that match the filtering criteria.\n",
    "\n",
    "- **`prepare_session_data_for_evaluation(df, extract_cols)`**: Groups spans by session, orders traces chronologically, and extracts specified columns into a tidy DataFrame format suitable for LLM evaluation. Transforms raw span data into structured session representations with user inputs and AI outputs organized by trace sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_filter(df: pd.DataFrame, column: str, operator: str, value: Any):\n",
    "    \"\"\"Lightweight filter helper supporting the ops used in this script.\"\"\"\n",
    "    if operator == \"==\":\n",
    "        return df[df[column] == value]\n",
    "    if operator == \"!=\":\n",
    "        return df[df[column] != value]\n",
    "    if operator == \"contains\":\n",
    "        return df[df[column].astype(str).str.contains(str(value), case=False, na=False)]\n",
    "    if operator == \"isna\":\n",
    "        return df[df[column].isna()]\n",
    "    if operator == \"notna\":\n",
    "        return df[df[column].notna()]\n",
    "    raise ValueError(f\"Unsupported operator: {operator}\")\n",
    "\n",
    "\n",
    "def filter_sessions_by_trace_criteria(\n",
    "    df: pd.DataFrame,\n",
    "    trace_filters: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "    span_filters: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return the *full* set of spans for any session that matches the filter.\n",
    "\n",
    "    This mirrors the behaviour of the Arize exporter version but operates on\n",
    "    a Phoenix span dataframe instead.\n",
    "    \"\"\"\n",
    "\n",
    "    trace_filters = trace_filters or {}\n",
    "    span_filters = span_filters or {}\n",
    "\n",
    "    filtered_df = df.copy()\n",
    "\n",
    "    # Apply trace-level filters first\n",
    "    for column, criteria in trace_filters.items():\n",
    "        for op, val in criteria.items():\n",
    "            filtered_df = _apply_filter(filtered_df, column, op, val)\n",
    "\n",
    "    # Apply span-level filters next\n",
    "    for column, criteria in span_filters.items():\n",
    "        for op, val in criteria.items():\n",
    "            filtered_df = _apply_filter(filtered_df, column, op, val)\n",
    "\n",
    "    # Guard – ensure session column exists (using the actual Phoenix column name)\n",
    "    if \"attributes.session.id\" not in filtered_df.columns:\n",
    "        raise ValueError(\n",
    "            \"Phoenix dataframe missing 'attributes.session.id' column – ensure your traces are session-aware.\"\n",
    "        )\n",
    "\n",
    "    matching_session_ids = filtered_df[\"attributes.session.id\"].unique().tolist()\n",
    "    print(f\"🔍  Found {len(matching_session_ids)} matching sessions\")\n",
    "\n",
    "    # Return *all* spans for those sessions so we have full context\n",
    "    return df[df[\"attributes.session.id\"].isin(matching_session_ids)].copy()\n",
    "\n",
    "\n",
    "# Re-implemented version of `prepare_session_data_for_evaluation` (simplified)\n",
    "\n",
    "\n",
    "def prepare_session_data_for_evaluation(\n",
    "    df: pd.DataFrame,\n",
    "    extract_cols: Optional[Dict[str, str]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Group spans by session & trace chronology → tidy df for evaluation.\"\"\"\n",
    "\n",
    "    extract_cols = extract_cols or {\n",
    "        \"user_inputs\": \"attributes.input.value\",\n",
    "        \"output_messages\": \"attributes.output.value\",\n",
    "    }\n",
    "\n",
    "    required_cols = {\"attributes.session.id\", \"context.trace_id\", \"start_time\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        missing = required_cols - set(df.columns)\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Group spans by session (using the actual Phoenix column name)\n",
    "    sessions = []\n",
    "    for session_id, session_df in df.groupby(\"attributes.session.id\"):\n",
    "        trace_ids = session_df[\"context.trace_id\"].unique().tolist()\n",
    "        # Order traces chronologically\n",
    "        trace_ids.sort(\n",
    "            key=lambda tid: session_df.loc[\n",
    "                session_df[\"context.trace_id\"] == tid, \"start_time\"\n",
    "            ].min()\n",
    "        )\n",
    "\n",
    "        session_dict: Dict[str, Any] = {\n",
    "            \"session.id\": session_id,\n",
    "            \"trace_count\": len(trace_ids),\n",
    "        }\n",
    "\n",
    "        for key, source_col in extract_cols.items():\n",
    "            trace_data = []\n",
    "            for idx, tid in enumerate(trace_ids, start=1):\n",
    "                values = (\n",
    "                    session_df.loc[session_df[\"context.trace_id\"] == tid, source_col]\n",
    "                    .dropna()\n",
    "                    .tolist()\n",
    "                )\n",
    "                if values:\n",
    "                    trace_data.append({str(idx): values})\n",
    "            session_dict[key] = trace_data\n",
    "\n",
    "        sessions.append(session_dict)\n",
    "\n",
    "    return pd.DataFrame(sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Sessions & Build Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_SPAN_FILTER = {\"parent_id\": {\"isna\": True}}\n",
    "\n",
    "print(\"🧹  Filtering to root spans and building evaluation dataset …\")\n",
    "root_df = filter_sessions_by_trace_criteria(\n",
    "    df=primary_df,\n",
    "    span_filters=ROOT_SPAN_FILTER,\n",
    ")\n",
    "\n",
    "sessions_df = prepare_session_data_for_evaluation(\n",
    "    df=root_df,\n",
    "    extract_cols={\n",
    "        \"user_inputs\": \"attributes.input.value\",\n",
    "        \"output_messages\": \"attributes.output.value\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"   → Prepared {len(sessions_df):,} sessions for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Prompt & Run LLM-based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_CORRECTNESS_PROMPT = \"\"\"\n",
    "You are a helpful AI bot that evaluates the effectiveness and correctness of an AI agent's session.\n",
    "\n",
    "A session consists of multiple traces (interactions) between a user and an AI system. I will provide you with:\n",
    "1. The user inputs that initiated each trace in the session, in chronological order\n",
    "2. The AI's output messages for each trace in the session, in chronological order\n",
    "3. The total number of traces in this session\n",
    "\n",
    "An effective and correct session:\n",
    "- Shows consistent understanding of user intentions across traces\n",
    "- Maintains context and coherence between interactions\n",
    "- Successfully achieves the overall user goals\n",
    "- Builds upon previous interactions in the conversation\n",
    "- Avoids unnecessary repetition or confusion\n",
    "\n",
    "##\n",
    "\n",
    "User Inputs:\n",
    "{user_inputs}\n",
    "\n",
    "Output Messages:\n",
    "{output_messages}\n",
    "\n",
    "##\n",
    "\n",
    "Evaluate the session based on the given criteria:\n",
    "- Assess whether the agent maintains coherence throughout the session\n",
    "- Analyze if the session progresses logically toward resolving user requests\n",
    "- Check if the agent effectively uses context from previous interactions\n",
    "\n",
    "Your response must be a single string, either `correct` or `incorrect`, and must not include any additional text.\n",
    "\n",
    "- Respond with `correct` if the session effectively accomplishes user goals with appropriate responses and coherence.\n",
    "- Respond with `incorrect` if the session shows confusion, inappropriate responses, or fails to accomplish user goals.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🤖  Running LLM evaluations … (this may take a while)\")\n",
    "\n",
    "# Set up the evaluation model\n",
    "openai_model = OpenAIModel(model=MODEL_NAME, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "rails = [\"correct\", \"incorrect\"]\n",
    "\n",
    "# Run the evaluation (using updated parameter name)\n",
    "results_df = llm_classify(\n",
    "    data=sessions_df,\n",
    "    template=SESSION_CORRECTNESS_PROMPT,\n",
    "    model=openai_model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    concurrency=20,\n",
    ")\n",
    "\n",
    "print(\"✅  LLM evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results & Log Back to Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Phoenix logging\n",
    "root_spans = (\n",
    "    primary_df.loc[primary_df[\"parent_id\"].isna(), [\"attributes.session.id\", \"context.span_id\"]]\n",
    "    .drop_duplicates(subset=[\"attributes.session.id\"], keep=\"first\")\n",
    "    .rename(columns={\"context.span_id\": \"span_id_root\", \"attributes.session.id\": \"session.id\"})\n",
    ")\n",
    "\n",
    "final_df = sessions_df.join(results_df).merge(\n",
    "    root_spans, left_on=\"session.id\", right_on=\"session.id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Prepare dataframe for Phoenix ingestion – Phoenix expects context.span_id index\n",
    "ingest_df = final_df.set_index(\"span_id_root\")[[\"label\", \"explanation\"]]\n",
    "# Phoenix expects the index to be named 'context.span_id'\n",
    "ingest_df.index.name = \"context.span_id\"\n",
    "\n",
    "# Log evaluations back to Phoenix\n",
    "if ingest_df.index.notna().all() and len(ingest_df):\n",
    "    print(\"\\n🚀  Logging evaluations back to Phoenix …\")\n",
    "    client.log_evaluations(\n",
    "        SpanEvaluations(\n",
    "            dataframe=ingest_df,\n",
    "            eval_name=\"Session Correctness\",\n",
    "        )\n",
    "    )\n",
    "    print(\"🎉  Evaluations logged! View them in the Phoenix UI → Evaluations tab.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Not logging to Phoenix – span_id mapping incomplete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
