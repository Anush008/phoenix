{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals Ergonomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to support evals for three different user flows:\n",
    "\n",
    "1. Running evals on an exported pandas dataframe and importing back into Phoenix,\n",
    "2. Running evals during LLM application execution via our callback system,\n",
    "3. Running evals post-hoc on a dataset of traces.\n",
    "\n",
    "Note: This notebook uses `phoenix.evals` everywhere for convenience, but we'll continue to keep `evals` in `experimental` for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running evals on an exported pandas dataframe and importing back into Phoenix\n",
    "\n",
    "The user is actively experimenting with their trace data in the form of a pandas dataframe. They wish to compute some evals, perhaps using our custom evaluators or perhaps using their own bespoke code, upload their evaluations, and see their evaluations reflected in Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import (\n",
    "    DefaultHallucinationClassificationConfig,\n",
    "    DefaultOpenAIGPT4RequestConfig,\n",
    "    DefaultRelevanceClassificationConfig,\n",
    "    LLMEvalConfig,\n",
    "    ManualRollingWindow,\n",
    "    PromptTemplate,\n",
    "    RequestConfig,\n",
    ")\n",
    "from phoenix.experimental.callbacks.langchain_tracer import OpenInferenceTracer\n",
    "\n",
    "session = px.launch_app()\n",
    "trace_df = session.export()\n",
    "tracer = OpenInferenceTracer()\n",
    "# Run your LLM application...\n",
    "\n",
    "trace_df = session.export_dataframe()\n",
    "\n",
    "# Method 1: Specify evals by name\n",
    "# run_evals is able to apply evals to the appropriate span kinds\n",
    "evals_df = run_evals(\n",
    "    trace_df,\n",
    "    model,\n",
    "    evals=[\n",
    "        LLMEvalConfig(\n",
    "            classification_configs=[\"relevance\", \"hallucination\"],\n",
    "            model=OpenAIChatModel(model_name=\"gpt-4\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Method 2: Specify evals by default configuration\n",
    "evals_df = run_evals(\n",
    "    trace_df,\n",
    "    model,\n",
    "    evals=[\n",
    "        LLMEvalConfig(\n",
    "            classification_configs=[\n",
    "                DefaultRelevanceClassificationConfig(),\n",
    "                DefaultHallucinationClassificationConfig(),\n",
    "            ],\n",
    "            model=OpenAIChatModel(model_name=\"gpt-4\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Method 3: Specify completely custom configurations.\n",
    "evals_df = run_evals(\n",
    "    trace_df,\n",
    "    evals=[\n",
    "        LLMEvalConfig(\n",
    "            classification_configs=[\n",
    "                RelevanceClassificationConfig(\n",
    "                    template=PromptTemplate(\n",
    "                        template_string=\"Query: {query}\\nReference: {reference}\\nResponse: \"\n",
    "                    ),\n",
    "                    rails=(\"relevant\", \"irrelevant\"),\n",
    "                    system_message='You are an assistant whose purpose is to classify a document as relevant or irrelevant to a query. You must respond with a single word, either \"relevant\" or \"irrelevant\".',\n",
    "                    query_variable_name=\"query\",\n",
    "                    reference_variable_name=\"reference\",\n",
    "                ),\n",
    "                HallucinationClassification(\n",
    "                    template=PromptTemplate(\n",
    "                        template_string=\"Query: {query}\\nReference: {reference}\\nResponse: {response}\\nHallucination: \"\n",
    "                    ),\n",
    "                    rails=(\"hallucinated\", \"grounded\"),\n",
    "                    system_message='You are an assistant whose purpose is to classify a response from an LLM as either a hallucination or a grounded response. You must respond with a single word, either \"hallucinated\" or \"grounded\".',\n",
    "                ),\n",
    "            ],\n",
    "            model=OpenAIChatModel(model_name=\"gpt-4\"),\n",
    "            request_config=RequestConfig(\n",
    "                rolling_window=ManualRollingWindow(\n",
    "                    rolling_window_duration,\n",
    "                    max_requests_per_window,\n",
    "                    max_tokens_per_window,\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Method 4: Using llm_eval_binary on a subset of the trace data\n",
    "trace_df = session.export_dataframe('span_kind == \"LLM\"')\n",
    "model = OpenAIModel(model_name=\"gpt-4\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"some prompt template the user is testing out with {context} and {question}\",\n",
    ")\n",
    "trace_df[\"relevance\"] = llm_eval_binary(\n",
    "    trace_df,\n",
    "    model,\n",
    "    template,\n",
    "    rails=[\"relevant\", \"irrelevant\"],\n",
    ")\n",
    "\n",
    "# import back into phoenix\n",
    "# accepts pandas dataframe or pandas series indexed with the same span IDs\n",
    "\n",
    "# Method 1: Importing a pandas series\n",
    "session.import_evals(trace_df[\"relevance\"])\n",
    "\n",
    "# Method 2: Importing a pandas dataframe\n",
    "session.import_evals(evals_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running evals during LLM application execution via our callback system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some users will want to execute our evals at application runtime by tying their evals to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.experimental.evals import LLMEvalConfig\n",
    "from phoenix.experimental.evals.models import AnthropicChatModel, OpenAIChatModel\n",
    "from phoenix.trace.langchain import LangChainInstrumentor, OpenInferenceTracer\n",
    "\n",
    "px.launch_app()\n",
    "\n",
    "# Method 1: Evaluations by name.\n",
    "tracer = OpenInferenceTracer(\n",
    "    evals=[\n",
    "        LLMEvalConfig(\n",
    "            evaluators=[\"hallucination\", \"relevance\", \"toxicity\"],\n",
    "            model=OpenAIChatModel(model_name=\"gpt-4\"),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "# in theory, someone could do evals with OpenAI and Anthropic at the same time if they wanted to\n",
    "# not really needed at the moment, but could be a reasonable ask if we wind up relying on features of particular apis\n",
    "tracer = OpenInferenceTracer(\n",
    "    evals=[\n",
    "        LLMEvalConfig(\n",
    "            evaluators=[\"hallucination\", \"relevance\"],\n",
    "            model=OpenAIChatModel(model_name=\"gpt-4\"),\n",
    "            request_config=DefaultOpenAIGPT4RequestConfig(),  # optional argument\n",
    "        ),\n",
    "        LLMEvalConfig(\n",
    "            evaluators=[\"toxicity\"],\n",
    "            model=AnthropicChatModel(model_name=\"claude-2\"),\n",
    "            request_config=DefaultAnthropicClaude2RequestConfig(),  # optional argument\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Method 2: Evaluations with custom configuration\n",
    "tracer = OpenInferenceTracer(\n",
    "    evals=[\n",
    "        LLMEvalConfig(\n",
    "            evaluators=[\n",
    "                DefaultRelevanceClassificationConfig(),\n",
    "                DefaultHallucinationClassificationConfig(),\n",
    "            ],\n",
    "            model=OpenAIChatModel(model_name=\"gpt-4\"),\n",
    "            request_config=DefaultOpenAIGPT4RequestConfig(),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "LangChainInstrumentor(tracer).instrument()\n",
    "\n",
    "# define your chain...\n",
    "\n",
    "# run your chain\n",
    "for query in queries:\n",
    "    chain.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- If someone is running our callbacks with one-click from LlamaIndex, how can they run with evals?\n",
    "- How does the user configure ranking metrics that are composite (i.e., require first that a classification is run and second that a score is computed)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running evals post-hoc\n",
    "\n",
    "Some users will want to run evals post-hoc an launch a phoenix dataset with their"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import LLMEvalConfig\n",
    "\n",
    "# load in some spans\n",
    "spans = ...\n",
    "\n",
    "# define a trace dataset\n",
    "ds = px.TraceDataset.from_spans(spans)\n",
    "\n",
    "# define evals in the same manner as above\n",
    "evals = [LLMEvalConfig(classification_configs=[...], model=...)]\n",
    "ds.run_evals(evals=evals)\n",
    "px.launch_app(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- How do we handle evaluations that require reference answers to compute?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
