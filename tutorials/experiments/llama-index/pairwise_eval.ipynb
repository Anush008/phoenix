{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKGDTl6hlIEu"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Pairwise Eval</h1>\n",
    "<h5 align=\"center\">ðŸ‘‰ See Llama-Index <a href=\"https://github.com/run-llama/llama_index/blob/a7c79201bbc5e195a0447ae557980791010b4747/docs/docs/examples/evaluation/pairwise_eval.ipynb\">notebook</a> for more info ðŸ‘ˆ</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pay9r0IAlIEu"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqqq arize-phoenix nest_asyncio openinference-instrumentation-llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEm1jNA_lIEu"
   },
   "source": [
    "# Enter OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYIwz_vMlIEv"
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from textwrap import shorten\n",
    "from time import time_ns\n",
    "from typing import Tuple\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from llama_index.core.evaluation import (\n",
    "    PairwiseComparisonEvaluator,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.experiments import evaluate_experiment, run_experiment\n",
    "from phoenix.experiments.types import Explanation, Score\n",
    "from phoenix.otel import register\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYQLgMn_lIEv"
   },
   "source": [
    "# Connect to Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PHOENIX_API_KEY\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"ðŸ”‘ Enter your Phoenix API key: \")\n",
    "\n",
    "if \"PHOENIX_COLLECTOR_ENDPOINT\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = getpass(\"ðŸ”‘ Enter your Phoenix Collector Endpoint: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6szxxwm2lIEv"
   },
   "source": [
    "# Instrument Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = register(endpoint=os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\"))\n",
    "LlamaIndexInstrumentor().instrument(skip_dep_check=True, tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLGWzjXelIEv"
   },
   "source": [
    "# Upload Dataset to Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 7\n",
    "category = \"creative_writing\"\n",
    "url = \"hf://datasets/databricks/databricks-dolly-15k/databricks-dolly-15k.jsonl\"\n",
    "df = pd.read_json(url, lines=True)\n",
    "df = df.loc[df.category == category, [\"instruction\", \"response\"]]\n",
    "df = df.sample(sample_size, random_state=42)\n",
    "dataset = px.Client().upload_dataset(\n",
    "    dataset_name=f\"{category}_{time_ns()}\",\n",
    "    dataframe=df,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9x15mHblIEv"
   },
   "source": [
    "# Dataset Can be Viewed as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpf934Q8lIEw"
   },
   "source": [
    "# Take a Look at the Data Structure of an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljg7O87_lIEw"
   },
   "source": [
    "# Define Task Function on Examples\n",
    "\n",
    "Task function can be either sync or async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def task(input):\n",
    "    return (await OpenAI(model=\"gpt-3.5-turbo\").acomplete(input[\"instruction\"])).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrvTBIPxlIEw"
   },
   "source": [
    "# Check that Task Can Run Successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "task_output = await task(example.input)\n",
    "print(shorten(json.dumps(task_output), width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRCI0dPulIEw"
   },
   "source": [
    "# Dry-Run Experiment\n",
    "\n",
    "On 3 randomly selected examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(dataset, task, dry_run=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtdQaeVTlIEw"
   },
   "source": [
    "# Experiment Results Can be Viewed as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiXwCmi7lIEw"
   },
   "source": [
    "# Take a Look at the Data Structure of an Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2hd86ltlIEw"
   },
   "source": [
    "# Define Evaluators For Each Experiment Run\n",
    "\n",
    "Evaluators can be sync or async.\n",
    "\n",
    "Function arguments `output` and `expected` refer to the attributes of the same name in the `ExperimentRun` data structure shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "async def pairwise(output, input, expected) -> Tuple[Score, Explanation]:\n",
    "    ans = await PairwiseComparisonEvaluator(llm=llm).aevaluate(\n",
    "        query=input[\"instruction\"],\n",
    "        response=output,\n",
    "        second_response=expected[\"response\"],\n",
    "    )\n",
    "    return ans.score, ans.feedback\n",
    "\n",
    "\n",
    "evaluators = [pairwise]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqHDp8jMlIEw"
   },
   "source": [
    "# Check that Evals Can Run Successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment[0]\n",
    "example = dataset.examples[run.dataset_example_id]\n",
    "for fn in evaluators:\n",
    "    _ = await fn(run.output, example.input, example.output)\n",
    "    print(fn.__qualname__)\n",
    "    print(shorten(json.dumps(_), width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H46FrV-lIEw"
   },
   "source": [
    "# Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = evaluate_experiment(experiment, evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVqlqKuxlIEw"
   },
   "source": [
    "# Evaluation Results Can be Viewed as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.get_evaluations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUlnUAGflIEw"
   },
   "source": [
    "# Run Task and Evals Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = run_experiment(dataset, task, evaluators)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
