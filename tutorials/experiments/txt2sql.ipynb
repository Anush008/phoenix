{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Experiments: Text2SQL</h1>\n",
    "\n",
    "Let's work through a Text2SQL use case where we are starting from scratch without a nice and clean dataset of questions, SQL queries, or expected responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"arize-phoenix>=10.0.0\" openai 'httpx<0.28' duckdb datasets pyarrow \"pydantic>=2.0.0\" nest_asyncio openinference-instrumentation-openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start a phoenix server. Note that this is not necessary if you have a phoenix server running already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeldking/work/phoenix/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "# px.launch_app().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup tracing for OpenAI as we will be using their API to perform the synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeldking/work/phoenix/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register()\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we can run async code in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's make sure we have our openai API key set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîë Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We are going to use the NBA dataset that information from 2014 - 2018. We will use DuckDB as our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 1,\n",
       " 'Team': 'ATL',\n",
       " 'Game': 1,\n",
       " 'Date': '10/29/14',\n",
       " 'Home': 'Away',\n",
       " 'Opponent': 'TOR',\n",
       " 'WINorLOSS': 'L',\n",
       " 'TeamPoints': 102,\n",
       " 'OpponentPoints': 109,\n",
       " 'FieldGoals': 40,\n",
       " 'FieldGoalsAttempted': 80,\n",
       " 'FieldGoals.': 0.5,\n",
       " 'X3PointShots': 13,\n",
       " 'X3PointShotsAttempted': 22,\n",
       " 'X3PointShots.': 0.591,\n",
       " 'FreeThrows': 9,\n",
       " 'FreeThrowsAttempted': 17,\n",
       " 'FreeThrows.': 0.529,\n",
       " 'OffRebounds': 10,\n",
       " 'TotalRebounds': 42,\n",
       " 'Assists': 26,\n",
       " 'Steals': 6,\n",
       " 'Blocks': 8,\n",
       " 'Turnovers': 17,\n",
       " 'TotalFouls': 24,\n",
       " 'Opp.FieldGoals': 37,\n",
       " 'Opp.FieldGoalsAttempted': 90,\n",
       " 'Opp.FieldGoals.': 0.411,\n",
       " 'Opp.3PointShots': 8,\n",
       " 'Opp.3PointShotsAttempted': 26,\n",
       " 'Opp.3PointShots.': 0.308,\n",
       " 'Opp.FreeThrows': 27,\n",
       " 'Opp.FreeThrowsAttempted': 33,\n",
       " 'Opp.FreeThrows.': 0.818,\n",
       " 'Opp.OffRebounds': 16,\n",
       " 'Opp.TotalRebounds': 48,\n",
       " 'Opp.Assists': 26,\n",
       " 'Opp.Steals': 13,\n",
       " 'Opp.Blocks': 9,\n",
       " 'Opp.Turnovers': 9,\n",
       " 'Opp.TotalFouls': 22}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n",
    "\n",
    "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "conn.register(\"nba\", data.to_pandas())\n",
    "\n",
    "conn.query(\"SELECT * FROM nba LIMIT 5\").to_df().to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Text2SQL\n",
    "\n",
    "Let's start by implementing a simple text2sql logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "client = openai.AsyncClient()\n",
    "\n",
    "columns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n",
    "\n",
    "# We will use GPT4o to start\n",
    "TASK_MODEL = \"gpt-4o\"\n",
    "CONFIG = {\"model\": TASK_MODEL}\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\"\n",
    "    f'{\",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\\n'\n",
    "    \"Write a SQL query corresponding to the user's request. Return just the query text, \"\n",
    "    \"with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "\n",
    "async def generate_query(input):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=TASK_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Team, COUNT(*) AS Wins\n",
      "FROM nba\n",
      "WHERE WINorLOSS = 'W'\n",
      "GROUP BY Team\n",
      "ORDER BY Wins DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "query = await generate_query(\"Who won the most games?\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, looks like the LLM is producing SQL! let's try running the query and see if we get the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Team': 'GSW', 'Wins': 265}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def execute_query(query):\n",
    "    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "execute_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation consists of three parts ‚Äî data, task, and scores. We'll start with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the data above as a versioned dataset in phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import phoenix as px\n",
    "\n",
    "# ds = px.Client().upload_dataset(\n",
    "#     dataset_name=\"nba-questions\",\n",
    "#     dataframe=pd.DataFrame([{\"question\": question} for question in questions]),\n",
    "#     input_keys=[\"question\"],\n",
    "#     output_keys=[],\n",
    "# )\n",
    "\n",
    "# If you have already uploaded the dataset, you can fetch it using the following line\n",
    "ds = px.Client().get_dataset(name=\"nba-questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the task. The task is to generate SQL queries from natural language questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def text2sql(question):\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the scores. We'll use the following simple scoring functions to see if the generated SQL queries are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if there are no sql execution errors\n",
    "\n",
    "\n",
    "def invalid_sql(output):\n",
    "    return 1.0 if output.get(\"error\") is None else 0.0\n",
    "\n",
    "\n",
    "# Test if the query has results\n",
    "def valid(output):\n",
    "    results = output.get(\"results\")\n",
    "    has_results = results is not None and len(results) > 0\n",
    "    return 1.0 if has_results else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the evaluation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDox/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDozOA==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 (100.0%) | ‚è≥ 00:03<00:00 |  2.76it/s\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:06<00:00 |  1.34s/it\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:03<00:00 |  1.60it/s | ?it/s\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 (100.0%) | ‚è≥ 00:01<00:00 |  9.79it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDozOA==\n",
      "\n",
      "Experiment Summary (06/06/25 05:50 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  invalid_sql  5         5        0.8\n",
      "1        valid  5         5        0.6\n",
      "\n",
      "Tasks Summary (06/06/25 05:50 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           5       5         0\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "\n",
    "# Define the task to run text2sql on the input question\n",
    "def task(input):\n",
    "    return text2sql(input[\"question\"])\n",
    "\n",
    "# Run the experiment    \n",
    "experiment = run_experiment(\n",
    "    ds, task=task, evaluators=[invalid_sql, valid], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! It looks like 3/5 of our queries are valid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "Now that we ran the initial evaluation, it looks like two of the results are valid, two produce SQL errors, and one is incorrect.\n",
    "\n",
    "- The incorrect query didn't seem to get the date format correct. That would probably be improved by showing a sample of the data to the model (e.g. few shot example).\n",
    "\n",
    "- There are is a binder error, which may also have to do with not understanding the data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve the prompt with few-shot examples and see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Team, COUNT(*) AS Wins\n",
      "FROM nba\n",
      "WHERE WINorLOSS = 'W' AND Date LIKE '%/15'\n",
      "GROUP BY Team\n",
      "ORDER BY Wins DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "samples = conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\\n\"\n",
    "    \"Column | Type | Example\\n\"\n",
    "    \"-------|------|--------\\n\"\n",
    "    f\"{sample_rows}\\n\"\n",
    "    \"\\n\"\n",
    "    \"Write a DuckDB SQL query corresponding to the user's request. \"\n",
    "    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "\n",
    "async def generate_query(input):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=TASK_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(await generate_query(\"Which team won the most games in 2015?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better! Finally, let's add a scoring function that compares the results, if they exist, with the expected results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDox/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDo0MQ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:02<00:00 |  1.82it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:03<00:00 |  1.38it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDo0MQ==\n",
      "\n",
      "Experiment Summary (06/06/25 05:51 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  invalid_sql  5         5        0.6\n",
      "1        valid  5         5        0.6\n",
      "\n",
      "Tasks Summary (06/06/25 05:51 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           5       5         0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RanExperiment' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m experiment = run_experiment(\n\u001b[32m     11\u001b[39m     ds, task=task, evaluators=[invalid_sql, valid], experiment_metadata=CONFIG\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Get the results\u001b[39;00m\n\u001b[32m     18\u001b[39m results = experiment.results()\n",
      "\u001b[31mAttributeError\u001b[39m: 'RanExperiment' object has no attribute 'run'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 (100.0%) | ‚è≥ 00:02<00:00 |  4.79it/s\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "\n",
    "# Define the task to run text2sql on the input question\n",
    "def task(input):\n",
    "    return text2sql(input[\"question\"])\n",
    "\n",
    "\n",
    "experiment = run_experiment(\n",
    "    ds, task=task, evaluators=[invalid_sql, valid], experiment_metadata=CONFIG\n",
    ")\n",
    "\n",
    "# Run the experiment\n",
    "experiment.run()\n",
    "\n",
    "# Get the results\n",
    "results = experiment.results()\n",
    "\n",
    "# Print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing. It looks like we removed one of the errors, and got a result for the incorrect query. Let's try out using LLM as a judge to see how well it can assess the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:12<00:00 |  2.85s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDozMQ==\n",
      "\n",
      "Experiment Summary (06/06/25 04:03 PM -0600)\n",
      "--------------------------------------------\n",
      "  evaluator  n  n_scores  avg_score\n",
      "0    is_sql  5         5        0.6\n",
      "\n",
      "Experiment Summary (06/06/25 04:03 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  5         5        0.6\n",
      "1     no_error  5         5        0.6\n",
      "\n",
      "Tasks Summary (06/06/25 04:03 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           5       5         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RanExperiment(id='RXhwZXJpbWVudDozMQ==', dataset_id='RGF0YXNldDox', dataset_version_id='RGF0YXNldFZlcnNpb246OQ==', repetitions=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.experiments import evaluate_experiment\n",
    "from phoenix.experiments.evaluators.llm_evaluators import LLMCriteriaEvaluator\n",
    "\n",
    "llm_evaluator = LLMCriteriaEvaluator(\n",
    "    name=\"is_sql\",\n",
    "    criteria=\"is_sql\",\n",
    "    description=\"the output is a valid SQL query and that it executes without errors\",\n",
    "    model=OpenAIModel(),\n",
    ")\n",
    "\n",
    "evaluate_experiment(experiment, evaluators=[llm_evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough the LLM agrees with our scoring. Pretty neat trick! This can come in useful when it's difficult to define a scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now have a simple text2sql pipeline that can be used to generate SQL queries from natural language questions. Since Phoenix has been tracing the entire pipeline, we can now use the Phoenix UI to convert the spans that generated successful queries into examples to use in **Golden Dataset** for regression testing!\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/golden_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating more data\n",
    "Now that we have a basic flow in place, let's generate some data. We're going to use the dataset itself to generate expected queries, and have a model describe the queries. This is a slightly more robust method than having it generate queries, because we'd expect a model to describe a query more accurately than generate one from scratch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:13<00:00 |  2.62s/it\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 (100.0%) | ‚è≥ 00:15<00:00 |  1.52s/it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sql': \"SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = 'W' GROUP BY Team ORDER BY Wins DESC;\",\n",
       " 'question': 'Which team has the most wins?'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    sql: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    questions: List[Question]\n",
    "\n",
    "\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "synthetic_data_prompt = f\"\"\"You are a SQL expert, and you are given a single table named nba with the following columns:\n",
    "\n",
    "Column | Type | Example\n",
    "-------|------|--------\n",
    "{sample_rows}\n",
    "\n",
    "Generate SQL queries that would be interesting to ask about this table. Return the SQL query as a string, as well as the\n",
    "question that the query answers.\"\"\"\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": synthetic_data_prompt,\n",
    "        }\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"generate_questions\",\n",
    "                \"description\": \"Generate SQL queries that would be interesting to ask about this table.\",\n",
    "                \"parameters\": Questions.model_json_schema(),\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"generate_questions\"}},\n",
    ")\n",
    "\n",
    "generated_questions = json.loads(response.choices[0].message.tool_calls[0].function.arguments)[\n",
    "    \"questions\"\n",
    "]\n",
    "generated_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query failed: SELECT Team, AVG(FieldGoals.) AS AvgFieldGoalPercentage FROM nba GROUP BY Team ORDER BY AvgFieldGoalPercentage DESC; Parser Error: syntax error at or near \")\"\n",
      "Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Which team has the most wins?',\n",
       " 'expected': {'results': [{'Team': 'GSW', 'Wins': 265},\n",
       "   {'Team': 'SAS', 'Wins': 230},\n",
       "   {'Team': 'HOU', 'Wins': 217},\n",
       "   {'Team': 'TOR', 'Wins': 215},\n",
       "   {'Team': 'CLE', 'Wins': 211},\n",
       "   {'Team': 'LAC', 'Wins': 202},\n",
       "   {'Team': 'BOS', 'Wins': 196},\n",
       "   {'Team': 'OKC', 'Wins': 195},\n",
       "   {'Team': 'POR', 'Wins': 185},\n",
       "   {'Team': 'WAS', 'Wins': 179},\n",
       "   {'Team': 'UTA', 'Wins': 177},\n",
       "   {'Team': 'ATL', 'Wins': 175},\n",
       "   {'Team': 'IND', 'Wins': 173},\n",
       "   {'Team': 'MIA', 'Wins': 170},\n",
       "   {'Team': 'MEM', 'Wins': 162},\n",
       "   {'Team': 'MIL', 'Wins': 160},\n",
       "   {'Team': 'CHI', 'Wins': 160},\n",
       "   {'Team': 'NOP', 'Wins': 157},\n",
       "   {'Team': 'CHO', 'Wins': 153},\n",
       "   {'Team': 'DET', 'Wins': 152},\n",
       "   {'Team': 'DAL', 'Wins': 149},\n",
       "   {'Team': 'DEN', 'Wins': 149},\n",
       "   {'Team': 'MIN', 'Wins': 123},\n",
       "   {'Team': 'SAC', 'Wins': 121},\n",
       "   {'Team': 'ORL', 'Wins': 114},\n",
       "   {'Team': 'NYK', 'Wins': 109},\n",
       "   {'Team': 'PHI', 'Wins': 108},\n",
       "   {'Team': 'BRK', 'Wins': 107},\n",
       "   {'Team': 'PHO', 'Wins': 107},\n",
       "   {'Team': 'LAL', 'Wins': 99}],\n",
       "  'error': None,\n",
       "  'query': \"SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = 'W' GROUP BY Team ORDER BY Wins DESC;\"},\n",
       " 'metadata': {'category': 'Generated'}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset = []\n",
    "for q in generated_questions:\n",
    "    try:\n",
    "        result = execute_query(q[\"sql\"])\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"input\": q[\"question\"],\n",
    "                \"expected\": {\n",
    "                    \"results\": result,\n",
    "                    \"error\": None,\n",
    "                    \"query\": q[\"sql\"],\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"category\": \"Generated\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"Query failed: {q['sql']}\", e)\n",
    "        print(\"Skipping...\")\n",
    "\n",
    "generated_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, let's crate a dataset with the new synthetic data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n"
     ]
    },
    {
     "ename": "DatasetUploadError",
     "evalue": "Dataset with the same name already exists: name='nba-golden-synthetic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/phoenix/src/phoenix/session/client.py:761\u001b[39m, in \u001b[36mClient._process_dataset_upload_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/phoenix/.venv/lib/python3.13/site-packages/httpx/_models.py:763\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '409 Conflict' for url 'http://127.0.0.1:6006/v1/datasets/upload?sync=true'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetUploadError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m synthetic_dataset = \u001b[43mpx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnba-golden-synthetic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerated_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexpected\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerated_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m;\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/phoenix/src/phoenix/session/client.py:531\u001b[39m, in \u001b[36mClient.upload_dataset\u001b[39m\u001b[34m(self, dataset_name, dataframe, csv_file_path, input_keys, output_keys, metadata_keys, inputs, outputs, metadata, dataset_description)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for type-checker\u001b[39;00m\n\u001b[32m    523\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._upload_tabular_dataset(\n\u001b[32m    524\u001b[39m         table,\n\u001b[32m    525\u001b[39m         dataset_name=dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    529\u001b[39m         dataset_description=dataset_description,\n\u001b[32m    530\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_upload_json_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_description\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/phoenix/src/phoenix/session/client.py:757\u001b[39m, in \u001b[36mClient._upload_json_dataset\u001b[39m\u001b[34m(self, dataset_name, inputs, outputs, metadata, dataset_description, action)\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müì§ Uploading dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    744\u001b[39m response = \u001b[38;5;28mself\u001b[39m._client.post(\n\u001b[32m    745\u001b[39m     url=\u001b[33m\"\u001b[39m\u001b[33mv1/datasets/upload\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    746\u001b[39m     headers={\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m   (...)\u001b[39m\u001b[32m    755\u001b[39m     params={\u001b[33m\"\u001b[39m\u001b[33msync\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m    756\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_dataset_upload_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/phoenix/src/phoenix/session/client.py:764\u001b[39m, in \u001b[36mClient._process_dataset_upload_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    763\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg := response.text:\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m DatasetUploadError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    766\u001b[39m data = response.json()[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mDatasetUploadError\u001b[39m: Dataset with the same name already exists: name='nba-golden-synthetic'"
     ]
    }
   ],
   "source": [
    "synthetic_dataset = px.Client().upload_dataset(\n",
    "    dataset_name=\"nba-golden-synthetic\",\n",
    "    inputs=[{\"question\": example[\"input\"]} for example in generated_dataset],\n",
    "    outputs=[example[\"expected\"] for example in generated_dataset],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDoxMg==/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoxMg==/compare?experimentId=RXhwZXJpbWVudDoyMg==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 (100.0%) | ‚è≥ 00:02<00:00 |  3.22it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoxMg==/compare?experimentId=RXhwZXJpbWVudDoyMg==\n",
      "\n",
      "Experiment Summary (06/06/25 03:43 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  8         8        0.5\n",
      "1     no_error  8         8        0.5\n",
      "\n",
      "Tasks Summary (06/06/25 03:43 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           8       8         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RanExperiment(id='RXhwZXJpbWVudDoyMg==', dataset_id='RGF0YXNldDoxMg==', dataset_version_id='RGF0YXNldFZlcnNpb246MTA=', repetitions=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\n",
    "    synthetic_dataset, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Now we have a rich dataset to work with and some failures to debug. From here, you could try to investigate whether some of the generated data needs improvement, or try tweaking the prompt to improve accuracy, or maybe even something more adventurous, like feed the errors back to the model and have it iterate on a better query. Most importantly, we have a good workflow in place to iterate on both the application and dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a smaller model\n",
    "Just for fun, let's wrap things up by trying out GPT-3.5-turbo. All we need to do is switch the model name, and run our Eval() function again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDoxMg==/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoxMg==/compare?experimentId=RXhwZXJpbWVudDoyMw==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 (100.0%) | ‚è≥ 00:02<00:00 |  7.54it/s\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 (100.0%) | ‚è≥ 00:02<00:00 |  3.12it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 (100.0%) | ‚è≥ 00:04<00:00 |  1.62it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoxMg==/compare?experimentId=RXhwZXJpbWVudDoyMw==\n",
      "\n",
      "Experiment Summary (06/06/25 03:43 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  8         8        1.0\n",
      "1     no_error  8         8        1.0\n",
      "\n",
      "Tasks Summary (06/06/25 03:43 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           8       8         0\n"
     ]
    }
   ],
   "source": [
    "TASK_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "experiment = run_experiment(\n",
    "    synthetic_dataset,\n",
    "    task=task,\n",
    "    evaluators=[no_error, has_results],\n",
    "    experiment_metadata={\"model\": TASK_MODEL},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! It looks like the smaller model is able to do decently well but we might want to ensure it follows instructions as well as a larger model. We can actually grab all the LLM spans from our previous GPT40 runs and use them to generate a OpenAI fine-tuning JSONL file!\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/fine_tining_nba.png\">\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/openai_ft.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we walked through the process of building a dataset for a text2sql application. We started with a few handwritten examples, and iterated on the dataset by using an LLM to generate more examples. We used the eval framework to track our progress, and iterated on the model and dataset to improve the results. Finally, we tried out a less powerful model to see if we could save cost or improve latency.\n",
    "\n",
    "Happy evaluations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
