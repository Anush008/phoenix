{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Relevance Classification Evals</h1>\n",
    "\n",
    "Arize provides tooling to evaluate LLM applications, including tools to determine the relevance or irrelevance of documents retrieved by retrieval-augmented generation (RAG) applications. This relevance is then used to measure the quality of each retrieval using ranking metrics such as precision@k. In order to determine whether each retrieved document is relevant or irrelevant to the corresponding query, our approach is straightforward: ask an LLM.\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to evaluate the performance of an LLM-assisted approach to relevance classification against information retrieval datasets with ground-truth relevance labels,\n",
    "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[experimental]==0.0.33rc6\" ipython matplotlib openai pycm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "from phoenix.experimental.evals import (\n",
    "    PromptTemplate,\n",
    "    download_benchmark_dataset,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against benchmark datasets of queries and retrieved documents with ground-truth relevance labels. Currently supported datasets include:\n",
    "\n",
    "- \"wiki_qa-train\"\n",
    "- \"ms_marco-v1.1-train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_benchmark_dataset(\n",
    "    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-test\"\n",
    ").sample(n=500, random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Relevance Classifications\n",
    "\n",
    "Run relevance classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(\n",
    "    columns={\n",
    "        \"query_text\": \"query\",\n",
    "        \"document_text\": \"reference\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def openai_functions_classify(\n",
    "    record: Dict[str, Any],\n",
    "    prompt_template: PromptTemplate,\n",
    "    classes: List[str],\n",
    "    model_name: str,\n",
    "    function_name: str,\n",
    "    function_description: str,\n",
    "    argument_name: str,\n",
    "    argument_description: str,\n",
    "    *,\n",
    "    system_message: Optional[str] = None,\n",
    "    require_explanation: bool = False,\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    if not all(variable_name in record for variable_name in prompt_template.variables):\n",
    "        raise ValueError(\"All prompt template variables must be present as keys in record.\")\n",
    "\n",
    "    user_message_content = prompt_template.format(\n",
    "        {variable_name: record[variable_name] for variable_name in prompt_template.variables}\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": user_message_content}]\n",
    "    if system_message:\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n",
    "    argument_data = {\n",
    "        argument_name: {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": argument_description,\n",
    "            \"enum\": classes,\n",
    "        },\n",
    "    }\n",
    "    if require_explanation:\n",
    "        argument_data[\"explanation\"] = {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A brief explanation of your reasoning for your answer.\",\n",
    "        }\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": function_name,\n",
    "            \"description\": function_description,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": argument_data,\n",
    "                \"required\": [argument_name],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        function_call={\"name\": function_name},\n",
    "    )\n",
    "    try:\n",
    "        response_message = response[\"choices\"][0][\"message\"]\n",
    "        assert response_message[\"function_call\"][\"name\"] == function_name\n",
    "        function_arguments = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
    "        return function_arguments[argument_name], function_arguments.get(\"explanation\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_string = \"\"\"You are comparing a reference text to a question and trying to determine if the reference text contains information relevant to answering the question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {query}\n",
    "    ************\n",
    "    [Reference text]: {reference}\n",
    "    [END DATA]\n",
    "\n",
    "Compare the question above to the reference text. You must determine whether the reference text contains information that can answer the question. Please focus on whether the very specific question can be answered by the information in the reference text.\"\"\"\n",
    "prompt_template = PromptTemplate(prompt_template_string)\n",
    "\n",
    "model_name = \"gpt-4\"\n",
    "\n",
    "relevance_classifications = []\n",
    "explanations = []\n",
    "for record in tqdm(df.to_dict(orient=\"records\")):\n",
    "    relevance_classification, explanation = openai_functions_classify(\n",
    "        record=record,\n",
    "        prompt_template=prompt_template,\n",
    "        classes=[\"relevant\", \"irrelevant\"],\n",
    "        model_name=model_name,\n",
    "        function_name=\"relevance\",\n",
    "        function_description=\"A function to record whether a reference text is relevant to a question.\",\n",
    "        argument_name=\"relevant\",\n",
    "        argument_description=\"A string indicating whether the reference text is relevant to the question.\",\n",
    "        require_explanation=True,\n",
    "    )\n",
    "    relevance_classifications.append(relevance_classification)\n",
    "    explanations.append(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifications\n",
    "\n",
    "Evaluate the predictions against human-labeled ground-truth relevance labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"relevant\"].map({True: \"relevant\", False: \"irrelevant\"}).tolist()\n",
    "predicted_labels = relevance_classifications\n",
    "classes = [\"relevant\", \"irrelevant\"]\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels, labels=classes))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=predicted_labels, classes=classes\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
