{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Evals\n",
    "\n",
    "This quickstart guide will show you through the basics of evaluating data from your LLM application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Phoenix Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install -q \"arize-phoenix[evals]\" arize-phoenix-otel\n",
    "pip install -q openai nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset\n",
    "The first thing you'll need is a dataset to evaluate. This could be your own collect or generated set of examples, or data you've exported from Phoenix traces. If you've already collected some trace data, this makes a great starting point.\n",
    "\n",
    "For the sake of this guide however, we'll download some pre-existing data to evaluate. Feel free to sub this with your own data, just be sure it includes the following columns:\n",
    "- reference\n",
    "- query\n",
    "- response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>() is a prefecture-level city in northweste...</td>\n",
       "      <td>Can  Fuyang and Gaozhou be found in the same p...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>() is a prefecture-level city in northweste...</td>\n",
       "      <td>Can  Fuyang and Gaozhou be found in the same p...</td>\n",
       "      <td>Yes, Fuyang and Gaozhou are in the same province.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"808\" was a success in the United States beco...</td>\n",
       "      <td>808 peaked at number eight on what?</td>\n",
       "      <td>Billboard\" Hot 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"808\" was a success in the United States beco...</td>\n",
       "      <td>808 peaked at number eight on what?</td>\n",
       "      <td>\"808\" peaked at number nine on \"Billboard\" Hot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Arms\" then made a comeback in 2017 reaching ...</td>\n",
       "      <td>Arms is a song by American singer-songwriter C...</td>\n",
       "      <td>Moana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reference  \\\n",
       "0     () is a prefecture-level city in northweste...   \n",
       "1     () is a prefecture-level city in northweste...   \n",
       "2   \"808\" was a success in the United States beco...   \n",
       "3   \"808\" was a success in the United States beco...   \n",
       "4   \"Arms\" then made a comeback in 2017 reaching ...   \n",
       "\n",
       "                                               query  \\\n",
       "0  Can  Fuyang and Gaozhou be found in the same p...   \n",
       "1  Can  Fuyang and Gaozhou be found in the same p...   \n",
       "2                808 peaked at number eight on what?   \n",
       "3                808 peaked at number eight on what?   \n",
       "4  Arms is a song by American singer-songwriter C...   \n",
       "\n",
       "                                            response  \n",
       "0                                                 no  \n",
       "1  Yes, Fuyang and Gaozhou are in the same province.  \n",
       "2                                 Billboard\" Hot 100  \n",
       "3  \"808\" peaked at number nine on \"Billboard\" Hot...  \n",
       "4                                              Moana  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals import download_benchmark_dataset\n",
    "SAMPLE_SIZE=10\n",
    "\n",
    "df = download_benchmark_dataset(\n",
    "    task=\"binary-hallucination-classification\", dataset_name=\"halueval_qa_data\"\n",
    ")\n",
    "df=df[:SAMPLE_SIZE]\n",
    "df = df.drop(columns=[\"is_hallucination\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and Log Results\n",
    "Set up evaluators (in this case for hallucinations and Q&A correctness), run the evaluations, and log the results to visualize them in Phoenix. We'll use OpenAI as our evaluation model for this example, but Phoenix also supports a number of other models. First, we need to add our OpenAI API key to our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82f39b7b26f46c99585ac252fd6d138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import OpenAIModel, HallucinationEvaluator, QAEvaluator\n",
    "from phoenix.evals import run_evals\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # This is needed for concurrency in notebook environments\n",
    "\n",
    "# Set your OpenAI API key\n",
    "eval_model = OpenAIModel(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "# Define your evaluators\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_evaluator = QAEvaluator(eval_model)\n",
    "\n",
    "# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n",
    "# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n",
    "# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\n",
    "df[\"context\"] = df[\"reference\"]\n",
    "df.rename(columns={\"query\": \"input\", \"response\":\"output\"}, inplace=True)\n",
    "assert all(column in df.columns for column in ['output', 'input', 'context', 'reference'])\n",
    "\n",
    "# Run the evaluators, each evaluator will return a dataframe with evaluation results\n",
    "# We upload the evaluation results to Phoenix in the next step\n",
    "hallucination_eval_df, qa_eval_df = run_evals(\n",
    "    dataframe=df,\n",
    "    evaluators=[hallucination_evaluator, qa_evaluator],\n",
    "    provide_explanation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the parameters used in run_evals above:\n",
    "- `dataframe` - a pandas dataframe that includes the data you want to evaluate. This could be spans exported from Phoenix, or data you've brought in from elsewhere. This dataframe must include the columns expected by the evaluators you are using. To see the columns expected by each built-in evaluator, check the corresponding page in the Using Phoenix Evaluators section.\n",
    "- `evaluators` - a list of built-in Phoenix evaluators to use.\n",
    "- `provide_explanations` - a binary flag that instructs the evaluators to generate explanations for their choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Your Evaluations\n",
    "Combine your evaluation results and explanations with your original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>context</th>\n",
       "      <th>hallucination_eval</th>\n",
       "      <th>hallucination_explanation</th>\n",
       "      <th>qa_eval</th>\n",
       "      <th>qa_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>() is a prefecture-level city in northweste...</td>\n",
       "      <td>Can  Fuyang and Gaozhou be found in the same p...</td>\n",
       "      <td>no</td>\n",
       "      <td>() is a prefecture-level city in northweste...</td>\n",
       "      <td>factual</td>\n",
       "      <td>The query asks if Fuyang and Gaozhou can be fo...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The reference text clearly states that Fuyang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>() is a prefecture-level city in northweste...</td>\n",
       "      <td>Can  Fuyang and Gaozhou be found in the same p...</td>\n",
       "      <td>Yes, Fuyang and Gaozhou are in the same province.</td>\n",
       "      <td>() is a prefecture-level city in northweste...</td>\n",
       "      <td>hallucinated</td>\n",
       "      <td>The reference text clearly states that Fuyang ...</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The reference text clearly states that Fuyang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"808\" was a success in the United States beco...</td>\n",
       "      <td>808 peaked at number eight on what?</td>\n",
       "      <td>Billboard\" Hot 100</td>\n",
       "      <td>\"808\" was a success in the United States beco...</td>\n",
       "      <td>factual</td>\n",
       "      <td>The query asks on which chart the song \"808\" p...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The question asks on which chart the song \"808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"808\" was a success in the United States beco...</td>\n",
       "      <td>808 peaked at number eight on what?</td>\n",
       "      <td>\"808\" peaked at number nine on \"Billboard\" Hot...</td>\n",
       "      <td>\"808\" was a success in the United States beco...</td>\n",
       "      <td>hallucinated</td>\n",
       "      <td>The reference text clearly states that \"808\" p...</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>The reference text clearly states that \"808\" p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Arms\" then made a comeback in 2017 reaching ...</td>\n",
       "      <td>Arms is a song by American singer-songwriter C...</td>\n",
       "      <td>Moana</td>\n",
       "      <td>\"Arms\" then made a comeback in 2017 reaching ...</td>\n",
       "      <td>factual</td>\n",
       "      <td>The query asks from which 2016 American 3D com...</td>\n",
       "      <td>correct</td>\n",
       "      <td>The question asks from which 2016 American 3D ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reference  \\\n",
       "0     () is a prefecture-level city in northweste...   \n",
       "1     () is a prefecture-level city in northweste...   \n",
       "2   \"808\" was a success in the United States beco...   \n",
       "3   \"808\" was a success in the United States beco...   \n",
       "4   \"Arms\" then made a comeback in 2017 reaching ...   \n",
       "\n",
       "                                               input  \\\n",
       "0  Can  Fuyang and Gaozhou be found in the same p...   \n",
       "1  Can  Fuyang and Gaozhou be found in the same p...   \n",
       "2                808 peaked at number eight on what?   \n",
       "3                808 peaked at number eight on what?   \n",
       "4  Arms is a song by American singer-songwriter C...   \n",
       "\n",
       "                                              output  \\\n",
       "0                                                 no   \n",
       "1  Yes, Fuyang and Gaozhou are in the same province.   \n",
       "2                                 Billboard\" Hot 100   \n",
       "3  \"808\" peaked at number nine on \"Billboard\" Hot...   \n",
       "4                                              Moana   \n",
       "\n",
       "                                             context hallucination_eval  \\\n",
       "0     () is a prefecture-level city in northweste...            factual   \n",
       "1     () is a prefecture-level city in northweste...       hallucinated   \n",
       "2   \"808\" was a success in the United States beco...            factual   \n",
       "3   \"808\" was a success in the United States beco...       hallucinated   \n",
       "4   \"Arms\" then made a comeback in 2017 reaching ...            factual   \n",
       "\n",
       "                           hallucination_explanation    qa_eval  \\\n",
       "0  The query asks if Fuyang and Gaozhou can be fo...    correct   \n",
       "1  The reference text clearly states that Fuyang ...  incorrect   \n",
       "2  The query asks on which chart the song \"808\" p...    correct   \n",
       "3  The reference text clearly states that \"808\" p...  incorrect   \n",
       "4  The query asks from which 2016 American 3D com...    correct   \n",
       "\n",
       "                                      qa_explanation  \n",
       "0  The reference text clearly states that Fuyang ...  \n",
       "1  The reference text clearly states that Fuyang ...  \n",
       "2  The question asks on which chart the song \"808...  \n",
       "3  The reference text clearly states that \"808\" p...  \n",
       "4  The question asks from which 2016 American 3D ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = df.copy()\n",
    "results_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\n",
    "results_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\n",
    "results_df[\"qa_eval\"] = qa_eval_df[\"label\"]\n",
    "results_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Log Results to Phoenix\n",
    "\n",
    "\n",
    "**Note:** You'll only be able to log evaluations to the Phoenix UI if you used a trace or span dataset exported from Phoenix as your dataset in this quickstart. If you've used your own outside dataset, you won't be able to log these results to Phoenix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log your evaluation results to Phoenix using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the evaluations\n",
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_eval_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view aggregate evaluation statistics, surface problematic spans, and understand the LLM's reason for each evaluation by simply reading the corresponding explanation. Phoenix seamlessly pinpoints the cause (irrelevant retrievals, incorrect parameterization of your LLM, etc.) of your LLM application's poor responses.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/evals/traces_with_evaluation_annotations.png\"></img>\n",
    "\n",
    "If you're interested in extending your evaluations to include relevance, explore our detailed Colab guide.\n",
    "Now that you're set up, read through the Concepts Section to get an understanding of the different components.\n",
    "If you want to learn how to accomplish a particular task, check out the How-To Guides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
