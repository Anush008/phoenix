{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NnhbGY7lVPE"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Self-Improving Agent</h1>\n",
    "\n",
    "This notebook demonstrates how to build a self-improving agent system that automatically gets better over time. The example uses a \"talk-to-your-data\" sales analytics agent.\n",
    "\n",
    "The notebook includes:\n",
    "* Building an agent with multiple tools (SQL queries, data analysis, visualization)\n",
    "* Manually instrumenting an agent using Phoenix decorators\n",
    "* Evaluating agent performance using LLM as a Judge and ground truth comparisons\n",
    "* Automated prompt optimization using meta-prompting\n",
    "* Creating evaluation datasets from production traces\n",
    "* Running experiments to validate prompt improvements\n",
    "* Deploying optimized prompts to production automatically\n",
    "* Self-improvement loop that continuously enhances agent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv pip install -q python-dotenv openai \"arize-phoenix>=8.8.0\" \"arize-phoenix-otel>=0.8.0\" openinference-instrumentation-openai python-dotenv duckdb \"openinference-instrumentation>=0.1.21\" tqdm dspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwtPY9bxlVPH"
   },
   "source": [
    "## Install Dependencies, Import Libraries, Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry.trace import StatusCode\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "\n",
    "from phoenix.client import Client as PhoenixClient\n",
    "from phoenix.otel import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "model = \"gpt-4o-mini\"\n",
    "project_name = \"self-improving-agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1K2CDJPlVPJ"
   },
   "source": [
    "## Enable Phoenix Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPFPr_ZClVPJ"
   },
   "source": [
    "Sign up for a free instance of [Phoenix Cloud](https://app.phoenix.arize.com) to get your API key. If you'd prefer, you can instead [self-host Phoenix](https://docs.arize.com/phoenix/deployment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCALLY HOSTED\n",
    "PHOENIX_ENDPOINT = \"http://0.0.0.0:6006/v1/traces\"\n",
    "\n",
    "# PHOENIX CLOUD\n",
    "# if os.getenv(\"PHOENIX_API_KEY\") is None:\n",
    "#    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\n",
    "# PHOENIX_ENDPOINT = \"https://app.phoenix.arize.com/v1/traces\"\n",
    "# os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = register(\n",
    "    project_name=project_name,\n",
    "    auto_instrument=True,\n",
    "    endpoint=PHOENIX_ENDPOINT,\n",
    ")\n",
    "\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Phoenix App if running in notebook\n",
    "import phoenix as px\n",
    "\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAOBb7RxlVPK"
   },
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Your agent will interact with a local database. Start by loading in that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_df = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/Store_Sales_Price_Elasticity_Promotions_Data.parquet\"\n",
    ")\n",
    "store_sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VBrVqHKlVPL"
   },
   "source": [
    "## Define the tools\n",
    "\n",
    "Now you can define your agent tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfURaEEVlVPL"
   },
   "source": [
    "### Tool 1: Database Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_GENERATION_PROMPT = \"\"\"\n",
    "Generate an SQL query based on a prompt. Do not reply with anything besides the SQL query.\n",
    "The prompt is: {prompt}\n",
    "\n",
    "The available columns are: {columns}\n",
    "The table name is: {table_name}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_sql_query(prompt: str, columns: list, table_name: str) -> str:\n",
    "    \"\"\"Generate an SQL query based on a prompt\"\"\"\n",
    "    formatted_prompt = SQL_GENERATION_PROMPT.format(\n",
    "        prompt=prompt, columns=columns, table_name=table_name\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@tracer.tool()\n",
    "def lookup_sales_data(prompt: str) -> str:\n",
    "    \"\"\"Implementation of sales data lookup from parquet file using SQL\"\"\"\n",
    "    try:\n",
    "        table_name = \"sales\"\n",
    "        # Read the parquet file into a DuckDB table\n",
    "        duckdb.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM store_sales_df\")\n",
    "\n",
    "        print(store_sales_df.columns)\n",
    "        print(table_name)\n",
    "        sql_query = generate_sql_query(prompt, store_sales_df.columns, table_name)\n",
    "        sql_query = sql_query.strip()\n",
    "        sql_query = sql_query.replace(\"```sql\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "        with tracer.start_as_current_span(\n",
    "            \"execute_sql_query\", openinference_span_kind=\"chain\"\n",
    "        ) as span:\n",
    "            span.set_input(value=sql_query)\n",
    "\n",
    "            # Execute the SQL query\n",
    "            result = duckdb.sql(sql_query).df()\n",
    "            span.set_output(value=str(result))\n",
    "            span.set_status(StatusCode.OK)\n",
    "        return result.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error accessing data: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = lookup_sales_data(\"Show me all the sales for store 1320 on November 1st, 2021\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvRQvLxvlVPM"
   },
   "source": [
    "### Tool 2: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationConfig(BaseModel):\n",
    "    chart_type: str = Field(..., description=\"Type of chart to generate\")\n",
    "    x_axis: str = Field(..., description=\"Name of the x-axis column\")\n",
    "    y_axis: str = Field(..., description=\"Name of the y-axis column\")\n",
    "    title: str = Field(..., description=\"Title of the chart\")\n",
    "\n",
    "\n",
    "@tracer.chain()\n",
    "def extract_chart_config(data: str, visualization_goal: str) -> dict:\n",
    "    \"\"\"Generate chart visualization configuration\n",
    "\n",
    "    Args:\n",
    "        data: String containing the data to visualize\n",
    "        visualization_goal: Description of what the visualization should show\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing line chart configuration\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Generate a chart configuration based on this data: {data}\n",
    "    The goal is to show: {visualization_goal}\"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=VisualizationConfig,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Extract axis and title info from response\n",
    "        content = response.choices[0].message.content\n",
    "\n",
    "        # Return structured chart config\n",
    "        return {\n",
    "            \"chart_type\": content.chart_type,\n",
    "            \"x_axis\": content.x_axis,\n",
    "            \"y_axis\": content.y_axis,\n",
    "            \"title\": content.title,\n",
    "            \"data\": data,\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"chart_type\": \"line\",\n",
    "            \"x_axis\": \"date\",\n",
    "            \"y_axis\": \"value\",\n",
    "            \"title\": visualization_goal,\n",
    "            \"data\": data,\n",
    "        }\n",
    "\n",
    "\n",
    "@tracer.chain()\n",
    "def create_chart(config: VisualizationConfig) -> str:\n",
    "    \"\"\"Create a chart based on the configuration\"\"\"\n",
    "    prompt = f\"\"\"Write python code to create a chart based on the following configuration.\n",
    "    Only return the code, no other text.\n",
    "    config: {config}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    code = response.choices[0].message.content\n",
    "    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    code = code.strip()\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "@tracer.tool()\n",
    "def generate_visualization(data: str, visualization_goal: str) -> str:\n",
    "    \"\"\"Generate a visualization based on the data and goal\"\"\"\n",
    "    config = extract_chart_config(data, visualization_goal)\n",
    "    code = create_chart(config)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = generate_visualization(example_data, \"A line chart of sales over each day in november.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6_Bsl_jlVPN"
   },
   "source": [
    "### Tool 3: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool()\n",
    "def analyze_sales_data(prompt: str, data: str) -> str:\n",
    "    \"\"\"Implementation of AI-powered sales data analysis\"\"\"\n",
    "    # Construct prompt based on analysis type and data subset\n",
    "    prompt = f\"\"\"Analyze the following data: {data}\n",
    "    Your job is to answer the following question: {prompt}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    analysis = response.choices[0].message.content\n",
    "    return analysis if analysis else \"No analysis could be generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = analyze_sales_data(\"What is the most popular product SKU?\", example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvMYbjN3lVPN"
   },
   "source": [
    "### Tool Schema:\n",
    "\n",
    "You'll need to pass your tool descriptions into your agent router. The following code allows you to easily do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools/functions that can be called by the model\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_sales_data\",\n",
    "            \"description\": \"Look up data from Store Sales Price Elasticity Promotions dataset\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"prompt\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unchanged prompt that the user provided.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"prompt\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"analyze_sales_data\",\n",
    "            \"description\": \"Analyze sales data to extract insights\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"data\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The lookup_sales_data tool's output.\",\n",
    "                    },\n",
    "                    \"prompt\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unchanged prompt that the user provided.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"data\", \"prompt\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"generate_visualization\",\n",
    "            \"description\": \"Generate Python code to create data visualizations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"data\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The lookup_sales_data tool's output.\",\n",
    "                    },\n",
    "                    \"visualization_goal\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The goal of the visualization.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"data\", \"visualization_goal\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# Dictionary mapping function names to their implementations\n",
    "tool_implementations = {\n",
    "    \"lookup_sales_data\": lookup_sales_data,\n",
    "    \"analyze_sales_data\": analyze_sales_data,\n",
    "    \"generate_visualization\": generate_visualization,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRBru3WtlVPO"
   },
   "source": [
    "## Save Router Prompt in Phoenix\n",
    "\n",
    "Saving prompts in Phoenix allows for easy version tracking of your prompts. For this example, since you'll be optimizing the router prompt, we'll save that as a Prompt in Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.completion_create_params import CompletionCreateParamsBase\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.client.types import PromptVersion\n",
    "\n",
    "params = CompletionCreateParamsBase(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"{{user_query}}\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt_name = \"self-improving-agent-router\"\n",
    "prompt_description = \"Router prompt for Self-Improving agent\"\n",
    "prompt = px.Client().prompts.create(\n",
    "    name=prompt_name,\n",
    "    version=PromptVersion.from_openai(params),\n",
    "    prompt_description=prompt_description,\n",
    ")\n",
    "\n",
    "px.Client().prompts.tags.create(prompt_version_id=prompt.id, name=\"production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQHylUxclVPO"
   },
   "source": [
    "## Agent logic\n",
    "\n",
    "With the tools defined, you're ready to define the main routing and tool call handling steps of your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain()\n",
    "def handle_tool_calls(tool_calls, messages):\n",
    "    for tool_call in tool_calls:\n",
    "        function = tool_implementations[tool_call.function.name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        result = function(**function_args)\n",
    "\n",
    "        messages.append({\"role\": \"tool\", \"content\": result, \"tool_call_id\": tool_call.id})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_production_router_prompt = PhoenixClient().prompts.get(\n",
    "    prompt_identifier=\"self-improving-agent-router\", tag=\"production\"\n",
    ")\n",
    "phoenix_production_router_prompt._template[\"messages\"][0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_main_span(messages):\n",
    "    print(\"Starting main span with messages:\", messages)\n",
    "\n",
    "    with tracer.start_as_current_span(\"AgentRun\", openinference_span_kind=\"agent\") as span:\n",
    "        span.set_input(value=messages)\n",
    "        ret = run_agent(messages)\n",
    "        print(\"Main span completed with return value:\", ret)\n",
    "        span.set_output(value=ret)\n",
    "        span.set_status(StatusCode.OK)\n",
    "        return ret\n",
    "\n",
    "\n",
    "def run_agent(messages):\n",
    "    print(\"Running agent with messages:\", messages)\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "        print(\"Converted string message to list format\")\n",
    "\n",
    "    # Check and add system prompt if needed\n",
    "    if not any(\n",
    "        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n",
    "    ):\n",
    "        phoenix_production_router_prompt = PhoenixClient().prompts.get(\n",
    "            prompt_identifier=\"self-improving-agent-router\", tag=\"production\"\n",
    "        )\n",
    "\n",
    "        system_message = None\n",
    "        for message in phoenix_production_router_prompt._template[\"messages\"]:\n",
    "            if message.get(\"role\") == \"system\":\n",
    "                system_message = message\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"No system message found saved prompt\")\n",
    "\n",
    "        # preppend system message\n",
    "        messages.insert(0, system_message)\n",
    "        print(\"Added system prompt to messages\")\n",
    "\n",
    "    while True:\n",
    "        # Router call span\n",
    "        print(\"Starting router call span\")\n",
    "        with tracer.start_as_current_span(\n",
    "            \"router_call\",\n",
    "            openinference_span_kind=\"chain\",\n",
    "        ) as span:\n",
    "            span.set_input(value=messages)\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "            )\n",
    "\n",
    "            messages.append(response.choices[0].message.model_dump())\n",
    "            tool_calls = response.choices[0].message.tool_calls\n",
    "            print(\"Received response with tool calls:\", bool(tool_calls))\n",
    "            span.set_status(StatusCode.OK)\n",
    "\n",
    "            if tool_calls:\n",
    "                # Tool calls span\n",
    "                print(\"Processing tool calls\")\n",
    "                messages = handle_tool_calls(tool_calls, messages)\n",
    "                span.set_output(value=tool_calls)\n",
    "            else:\n",
    "                print(\"No tool calls, returning final response\")\n",
    "                span.set_output(value=response.choices[0].message.content)\n",
    "\n",
    "                return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie4tkZsWlVPP"
   },
   "source": [
    "## Run the agent\n",
    "\n",
    "Your agent is now good to go! Let's try it out with some example questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = start_main_span([{\"role\": \"user\", \"content\": \"Create a line chart showing sales in 2021\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this will take ~15 minutes to run\n",
    "\n",
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\",\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"What percentage of items were sold on promotion?\",\n",
    "    \"Plot daily sales volume over time\",\n",
    "    \"What was the average transaction value?\",\n",
    "    \"Create a box plot of transaction values\",\n",
    "    \"Which products were frequently purchased together?\",\n",
    "    \"Plot a line graph showing the sales trend over time with a 7-day moving average\",\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    phoenix_production_router_prompt = PhoenixClient().prompts.get(\n",
    "        prompt_identifier=\"self-improving-agent-router\", tag=\"production\"\n",
    "    )\n",
    "    interpolated_messages = phoenix_production_router_prompt.format(\n",
    "        variables={\"user_query\": question}\n",
    "    ).messages\n",
    "    print(\"MESSAGES: \", interpolated_messages)\n",
    "    try:\n",
    "        ret = start_main_span(interpolated_messages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P34m9fgplVPQ"
   },
   "source": [
    "![Agent Traces](https://storage.googleapis.com/arize-phoenix-assets/assets/images/agent-traces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNPYr0t1lVPQ"
   },
   "source": [
    "# Test the Agent in Development\n",
    "\n",
    "Before deploying your agent, you can first test it on a series of test cases. You'll need to initially either generate or source these test cases yourself, but in future rounds, this will be automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIInstrumentor().uninstrument()  # Uninstrument the OpenAI client to avoid capturing LLM as a Judge evaluation calls in your same project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.evals import TOOL_CALLING_PROMPT_TEMPLATE, OpenAIModel, llm_classify\n",
    "from phoenix.experiments import run_experiment\n",
    "from phoenix.experiments.types import Example\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_client = px.Client()\n",
    "eval_model = OpenAIModel(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71j1M9AJlVPR"
   },
   "source": [
    "### Function Calling Evals using Ground Truth\n",
    "\n",
    "In order to run a test on the ground truth data effectively, you can use an Experiment.\n",
    "\n",
    "Experiments follow a standard step-by-step process in Phoenix:\n",
    "1. Create a dataset of test cases, and optionally, expected outputs\n",
    "2. Create a task to run on each test case - usually this is invoking your agent or a specifc step of it\n",
    "3. Create evaluator(s) to run on each output of your task\n",
    "4. Visualize results in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id = str(uuid.uuid4())\n",
    "\n",
    "# Create a list of tuples with input_messages and next_tool_call\n",
    "data = [\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Plot daily sales volume over time\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_1\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Plot daily sales volume over time\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_1\",\n",
    "                \"content\": \"     Sold_Date  Daily_Sales_Volume\\n0   2021-11-01              1021.0\\n1   2021-11-02              1035.0\\n2   2021-11-03               900.0\",\n",
    "            },\n",
    "        ],\n",
    "        \"analyze_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"What were the top selling products last month?\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Show me the relationship between promotions and sales\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_2\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get promotion and sales data\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_2\",\n",
    "                \"content\": \"   On_Promo  Total_Sale_Value\\n0         0          1245678.50\\n1         1           987654.32\",\n",
    "            },\n",
    "        ],\n",
    "        \"analyze_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Calculate the price elasticity for SKU 6172800\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Create a bar chart of sales by store\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_3\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get sales by store\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_3\",\n",
    "                \"content\": \"   Store_Number  Total_Sales\\n0          1320      56849.99\\n1          2310      37900.00\\n2          3080      18950.00\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_visualization\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Find trends in seasonal sales patterns\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"How does product class code affect sales volume?\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_4\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get sales volume by product class code\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_4\",\n",
    "                \"content\": \"   Product_Class_Code  Total_Qty_Sold\\n0               22875             7\\n1               34567            12\\n2               45678            23\",\n",
    "            },\n",
    "        ],\n",
    "        \"analyze_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Generate a scatter plot of price vs quantity sold\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Which stores have the highest promotion effectiveness?\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_5\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get promotion and sales data by store\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_5\",\n",
    "                \"content\": \"   Store_Number  Promo_Sales  Regular_Sales  Effectiveness\\n0          1320      12500.0        10000.0           1.25\\n1          2310      15000.0        10000.0           1.50\\n2          3080       9000.0        10000.0           0.90\",\n",
    "            },\n",
    "        ],\n",
    "        \"no tool called\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Compare sales performance between 2020 and 2021\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataframe = pd.DataFrame(data, columns=[\"input_messages\", \"next_tool_call\"])\n",
    "\n",
    "dataset = px_client.upload_dataset(\n",
    "    dataframe=dataframe,\n",
    "    dataset_name=f\"tool_calling_ground_truth_{id}\",\n",
    "    input_keys=[\"input_messages\"],\n",
    "    output_keys=[\"next_tool_call\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBLPKZRulVPS"
   },
   "source": [
    "For your task, you can simply run just the router call of your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_router_step(example: Example) -> str:\n",
    "    input_messages = example.input.get(\"input_messages\")\n",
    "\n",
    "    phoenix_production_router_prompt = PhoenixClient().prompts.get(\n",
    "        prompt_identifier=\"self-improving-agent-router\", tag=\"production\"\n",
    "    )\n",
    "\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": phoenix_production_router_prompt,\n",
    "    }\n",
    "\n",
    "    # Replace the system message in input_messages with our production router prompt\n",
    "    # or add it if no system message exists\n",
    "    system_message_index = None\n",
    "\n",
    "    for i, message in enumerate(input_messages):\n",
    "        if message.get(\"role\") == \"system\":\n",
    "            system_message_index = i\n",
    "            break\n",
    "\n",
    "    if system_message_index is not None:\n",
    "        # Replace existing system message\n",
    "        input_messages[system_message_index] = system_prompt\n",
    "    else:\n",
    "        # Add system message if none exists\n",
    "        input_messages.insert(0, system_prompt)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=input_messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.tool_calls is None:\n",
    "        return \"no tool called\"\n",
    "\n",
    "    tool_calls = []\n",
    "    for tool_call in response.choices[0].message.tool_calls:\n",
    "        tool_calls.append(tool_call.function.name)\n",
    "    return tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQrWQt2xlVPT"
   },
   "source": [
    "Your evaluator can also be simple, since you have expected outputs. If you didn't have those expected outputs, you could instead use an LLM as a Judge here, or even basic code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tools_match(expected: str, output: str) -> bool:\n",
    "    if not isinstance(output, list):\n",
    "        return False\n",
    "\n",
    "    # Check if all expected tools are in output and no additional tools are present\n",
    "    expected_tools = expected.get(\"next_tool_call\").split(\", \")\n",
    "    expected_set = set(expected_tools)\n",
    "    output_set = set(output)\n",
    "\n",
    "    # Return True if the sets are identical (same elements, no extras)\n",
    "    return expected_set == output_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(\n",
    "    dataset,\n",
    "    run_router_step,\n",
    "    evaluators=[tools_match],\n",
    "    experiment_name=\"Tool Calling Eval\",\n",
    "    experiment_description=\"Evaluating the tool calling step of the agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0sohECelVPT"
   },
   "source": [
    "## Optimize your Agent in Development\n",
    "\n",
    "Now you can optimize your agent's routing prompt based on the labeled data you've created so far. To do this, you'll use meta prompting, a technique that involves prompting a model to generate a better prompt based on previous inputs, outputs, and expected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_router_with_meta_prompting(trainset):\n",
    "    \"\"\"Use meta prompting to optimize the router prompt based on training examples\"\"\"\n",
    "\n",
    "    # Get the current production router prompt\n",
    "    phoenix_production_router_prompt = PhoenixClient().prompts.get(\n",
    "        prompt_identifier=\"self-improving-agent-router\", tag=\"production\"\n",
    "    )\n",
    "\n",
    "    # Extract the original prompt content\n",
    "    original_prompt = None\n",
    "    for message in phoenix_production_router_prompt._template[\"messages\"]:\n",
    "        if message.get(\"role\") == \"system\":\n",
    "            original_prompt = message.get(\"content\")\n",
    "            break\n",
    "\n",
    "    if not original_prompt:\n",
    "        raise ValueError(\"No system message found in saved prompt\")\n",
    "\n",
    "    # Format examples for meta prompting\n",
    "    example_strings = []\n",
    "    for example in trainset[:10]:  # Use first 10 examples\n",
    "        example_str = f\"Input: {example['input']}\\nOutput: {example['output']}\\nExpected Output: {example['expected']}\"\n",
    "        example_strings.append(example_str)\n",
    "\n",
    "    examples_text = \"\\n\\n\".join(example_strings)\n",
    "\n",
    "    # Meta prompt for router optimization\n",
    "    meta_prompt = f\"\"\"\n",
    "    You are an expert prompt engineer. You are given a router prompt for an AI agent that decides which tool to call based on user queries, along with examples of inputs, actual outputs, and expected outputs.\n",
    "\n",
    "    Your job is to generate an improved router prompt that will better route user queries to the correct tools.\n",
    "\n",
    "    Available tools:\n",
    "    1. lookup_sales_data: Use for retrieving raw sales data from the database\n",
    "    2. analyze_sales_data: Use for complex analysis and insights from sales data\n",
    "    3. generate_visualization: Use when users want charts, graphs, or visual representations\n",
    "\n",
    "    Here are examples where the current prompt made mistakes:\n",
    "\n",
    "    {examples_text}\n",
    "\n",
    "    Here is the current router prompt:\n",
    "\n",
    "    {original_prompt}\n",
    "\n",
    "    Please generate an improved router prompt that addresses the issues shown in the examples. The prompt should:\n",
    "    1. Better distinguish between data lookup, analysis, and visualization requests\n",
    "    2. Be more specific about when to use each tool\n",
    "    3. Include clear decision criteria\n",
    "\n",
    "    Only return the improved prompt text, nothing else:\n",
    "    \"\"\"\n",
    "\n",
    "    # Call OpenAI to generate improved prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=[{\"role\": \"user\", \"content\": meta_prompt}], temperature=0.1\n",
    "    )\n",
    "\n",
    "    improved_prompt = response.choices[0].message.content.strip()\n",
    "    return improved_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainset for development\n",
    "trainset = []\n",
    "\n",
    "for input_messages, next_tool_call in dataframe.values:\n",
    "    trainset.append(\n",
    "        {\n",
    "            \"input\": input_messages,\n",
    "            \"output\": next_tool_call,  #  Actual tool call made\n",
    "            \"expected\": next_tool_call,  # Ground truth as expected\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(trainset[:3])\n",
    "\n",
    "new_prompt = optimize_router_with_meta_prompting(trainset)\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = CompletionCreateParamsBase(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": new_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"{user_query}\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# This will update the existing prompt in Phoenix\n",
    "prompt_name = \"self-improving-agent-router\"\n",
    "prompt = px.Client().prompts.create(\n",
    "    name=prompt_name,\n",
    "    prompt_description=\"Router prompt for the self-improving agent\",\n",
    "    version=PromptVersion.from_openai(params),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tag for a prompt version\n",
    "px.Client().prompts.tags.create(\n",
    "    prompt_version_id=prompt.id, name=\"production\", description=\"Ready for production environment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcwBfh-GlVPU"
   },
   "source": [
    "## Evaluate your Agent in Production\n",
    "\n",
    "Now\n",
    "\n",
    "It follows a standard pattern:\n",
    "1. Export traces from Phoenix\n",
    "2. Prepare those exported traces in a dataframe with the correct columns\n",
    "3. Use `llm_classify` to run a standard template across each row of that dataframe and produce an eval label\n",
    "4. Upload the results back into Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_calls():\n",
    "    query = (\n",
    "        SpanQuery()\n",
    "        .where(\n",
    "            \"span_kind == 'LLM'\",\n",
    "        )\n",
    "        .select(question=\"input.value\", output_messages=\"llm.output_messages\")\n",
    "    )\n",
    "\n",
    "    # The Phoenix Client can take this query and return the dataframe.\n",
    "    tool_calls_df = px.Client().query_spans(query, project_name=project_name, timeout=None)\n",
    "    tool_calls_df.dropna(subset=[\"output_messages\"], inplace=True)\n",
    "\n",
    "    def get_tool_call(outputs):\n",
    "        if outputs[0].get(\"message\").get(\"tool_calls\"):\n",
    "            return (\n",
    "                outputs[0]\n",
    "                .get(\"message\")\n",
    "                .get(\"tool_calls\")[0]\n",
    "                .get(\"tool_call\")\n",
    "                .get(\"function\")\n",
    "                .get(\"name\")\n",
    "            )\n",
    "        else:\n",
    "            return \"No tool used\"\n",
    "\n",
    "    tool_calls_df[\"tool_call\"] = tool_calls_df[\"output_messages\"].apply(get_tool_call)\n",
    "    tool_definitions_list = [tools] * len(tool_calls_df)\n",
    "    tool_calls_df[\"tool_definitions\"] = tool_definitions_list\n",
    "    return tool_calls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tool_calls(dataframe):\n",
    "    tool_call_eval = llm_classify(\n",
    "        data=dataframe,\n",
    "        template=TOOL_CALLING_PROMPT_TEMPLATE,\n",
    "        rails=[\"correct\", \"incorrect\"],\n",
    "        model=eval_model,\n",
    "        provide_explanation=True,\n",
    "    )\n",
    "\n",
    "    tool_call_eval[\"score\"] = tool_call_eval.apply(\n",
    "        lambda x: 1 if x[\"label\"] == \"correct\" else 0, axis=1\n",
    "    )\n",
    "\n",
    "    return tool_call_eval, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_log_tool_calls():\n",
    "    tool_calls_df = get_tool_calls()\n",
    "    tool_call_eval, dataframe = eval_tool_calls(tool_calls_df)\n",
    "    px.Client().log_evaluations(\n",
    "        SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval),\n",
    "    )\n",
    "\n",
    "    # Merge the evaluation results with the original dataframe on context.span_id\n",
    "    merged_df = pd.merge(tool_call_eval, dataframe, left_index=True, right_index=True, how=\"inner\")\n",
    "\n",
    "    # Return both the evaluation results and the merged dataframe\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_eval = eval_and_log_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_OTgCRklVPV"
   },
   "source": [
    "You should now see eval labels in Phoenix.\n",
    "\n",
    "# ![Function Calling Evals](https://storage.googleapis.com/arize-phoenix-assets/assets/images/function-calling-evals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGdcO-KVlVPV"
   },
   "source": [
    "## Creating the Automated Loop\n",
    "\n",
    "Now you can combine each of these pieces into a single loop in production. That loop will:\n",
    "1. Evaluate the production data at scale using LLM as a Judge\n",
    "2. Extra the correct application runs, and create a new trainset saved in Phoenix\n",
    "3. Pass that trainset into DSPy to generate a newly optimized prompt\n",
    "4. Run an experiment to benchmark the new prompt on previous dev data\n",
    "5. Ask the user whether to apply the new prompt and save it as the production prompt in Phoenix. This step could be automated instead to check against previous experiment benchmarks and auto-apply if this new variant exceeds them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainset(tool_call_eval):\n",
    "    trainset = []\n",
    "    for _, row in tool_call_eval.iterrows():\n",
    "        if row[\"label\"] == \"correct\":\n",
    "            trainset.append(\n",
    "                {\"input\": row[\"question\"], \"output\": row[\"tool_call\"], \"expected\": row[\"tool_call\"]}\n",
    "            )\n",
    "        else:\n",
    "            trainset.append(\n",
    "                {\n",
    "                    \"input\": row[\"question\"],\n",
    "                    \"output\": row[\"tool_call\"],\n",
    "                    \"expected\": \"correct_tool_call\",\n",
    "                }\n",
    "            )\n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trainset(trainset):\n",
    "    trainset_df = pd.DataFrame(trainset)\n",
    "    px.Client().upload_dataset(\n",
    "        dataframe=trainset_df,\n",
    "        dataset_name=\"self-improving-agent-trainset-{}\".format(uuid.uuid4()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_router(trainset):\n",
    "    return optimize_router_with_meta_prompting(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    experiment = run_experiment(\n",
    "        dataset,\n",
    "        run_router_step,\n",
    "        evaluators=[tools_match],\n",
    "        experiment_name=\"Tool Calling Eval\",\n",
    "        experiment_description=\"Evaluating the tool calling step of the agent\",\n",
    "    )\n",
    "    return experiment.eval_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt(prompt):\n",
    "    params = CompletionCreateParamsBase(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=tools,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": new_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{user_query}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # This will update the existing prompt in Phoenix\n",
    "    prompt_name = \"self-improving-agent-router\"\n",
    "    prompt = px.Client().prompts.create(\n",
    "        name=prompt_name,\n",
    "        prompt_description=\"Router prompt for the self-improving agent\",\n",
    "        version=PromptVersion.from_openai(params),\n",
    "    )\n",
    "\n",
    "    px.Client().prompts.tags.create(\n",
    "        prompt_version_id=prompt.id,\n",
    "        name=\"production\",\n",
    "        description=\"Ready for production environment\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_call_eval = eval_and_log_tool_calls()\n",
    "\n",
    "\n",
    "def automated_loop():\n",
    "    tool_call_eval = eval_and_log_tool_calls()\n",
    "    trainset = create_trainset(tool_call_eval)\n",
    "    save_trainset(trainset)\n",
    "    new_prompt = optimize_router(trainset)\n",
    "    experiment_results = run_experiment()\n",
    "    print(experiment_results.eval_summaries())\n",
    "    # Ask user if they want to apply the new prompt\n",
    "    apply_prompt = input(\"Do you want to apply the new prompt? (yes/no): \")\n",
    "\n",
    "    if apply_prompt.lower() not in [\"yes\", \"y\"]:\n",
    "        print(\"Prompt update cancelled.\")\n",
    "        return\n",
    "\n",
    "    print(\"Applying new prompt...\")\n",
    "    save_prompt(new_prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
